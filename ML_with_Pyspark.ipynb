{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_with_Pyspark",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJXHuKAo8WpT",
        "outputId": "0517c61e-3806-4787-8661-b9a08b6c4715"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 70kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 49.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=ea2db94820a63687680ca27e2f5f1bbacba5f018ce184fa736e7c52bdd6492c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTLyfiMj8fPW"
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark import SparkContext\r\n",
        "sc = SparkContext()\r\n",
        "spark = SparkSession.builder.master('local[*]').appName('first_spark_application').getOrCreate()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz0HGliZ_NxC",
        "outputId": "e5346dcd-7f96-40eb-d9e6-ce3bda071d7a"
      },
      "source": [
        "# Filter missing data\r\n",
        "\r\n",
        "df = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "df.show(5)\r\n",
        "print(f'The dataframe has {df.count()} observations with {len(df.columns)} columns.')\r\n",
        "\r\n",
        "# How many missing values\r\n",
        "print(f'There are {df.where(\"delay IS NULL\").count()} missing observations in \"flights.csv\"')\r\n",
        "print(f'You can get the same value using either .where() or .either(): {df.filter(\"delay IS NULL\").count()}.')\r\n",
        "\r\n",
        "# Drop observations with missing values only form the \"delay\" column\r\n",
        "print(f'We now have {df.where(\"delay IS NOT NULL\").count()} observations left.')\r\n",
        "print(f'If we drop missing values from any column we have {df.dropna().count()} left.')\r\n",
        "print(\"This means only the 'delay' column having missing values. \\n\")\r\n",
        "\r\n",
        "# Create a kilometers column\r\n",
        "from pyspark.sql.functions import round\r\n",
        "df = df.withColumn(\"kilometre\", df[\"mile\"]*1.609344)\r\n",
        "df = df.withColumn(\"kilometre_round\", round(df[\"mile\"]*1.609344).cast(\"integer\"))\r\n",
        "df[[\"kilometre\", \"kilometre_round\"]].show(3)\r\n",
        "\r\n",
        "from pyspark.sql.functions import when\r\n",
        "df = df.withColumn(\"label\", when(df[\"delay\"] > 15, 1).otherwise(0))\r\n",
        "df.show(5)\r\n",
        "\r\n",
        "# Indexing categorical data\r\n",
        "from pyspark.ml.feature import StringIndexer\r\n",
        "\r\n",
        "df = StringIndexer(inputCol=\"carrier\",  outputCol=\"carrier_index\").fit(df).transform(df)\r\n",
        "df = StringIndexer(inputCol=\"org\", outputCol=\"org_index\").fit(df).transform(df).drop(\"kilometre_round\")\r\n",
        "df.show(5)\r\n",
        "print(\"We can check that the index of zero has the most observations.\")\r\n",
        "print(\"Carrier 'UA' has the index 0 and have observations = \", df.where('carrier = \"UA\"').count())\r\n",
        "print(\"Carrier 'US' has the index 6 and have observations = \", df.where('carrier = \"US\"').count())\r\n",
        "\r\n",
        "# Vector Assembler\r\n",
        "from pyspark.ml.feature import VectorAssembler\r\n",
        "assembler = VectorAssembler(inputCols=[\"mon\", \"dom\", \"dow\", \"carrier_index\", \"org_index\", \"kilometre\", \"depart\", \"duration\"], outputCol='features')\r\n",
        "df = assembler.transform(df)\r\n",
        "df[[\"carrier_index\", \"org_index\", \"duration\", \"features\"]].show(5, truncate=False)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
            "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
            "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
            "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
            "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "The dataframe has 50000 observations with 10 columns.\n",
            "There are 2978 missing observations in \"flights.csv\"\n",
            "You can get the same value using either .where() or .either(): 2978.\n",
            "We now have 47022 observations left.\n",
            "If we drop missing values from any column we have 47022 left.\n",
            "This means only the 'delay' column having missing values. \n",
            "\n",
            "+------------------+---------------+\n",
            "|         kilometre|kilometre_round|\n",
            "+------------------+---------------+\n",
            "|       3464.917632|           3465|\n",
            "|508.55270400000006|            509|\n",
            "|        542.348928|            542|\n",
            "+------------------+---------------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+---------------+-----+\n",
            "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|         kilometre|kilometre_round|label|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+---------------+-----+\n",
            "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|       3464.917632|           3465|    0|\n",
            "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|508.55270400000006|            509|    1|\n",
            "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|        542.348928|            542|    0|\n",
            "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|       1989.149184|           1989|    0|\n",
            "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|        415.210752|            415|    0|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+---------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+-----+-------------+---------+\n",
            "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|         kilometre|label|carrier_index|org_index|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+-----+-------------+---------+\n",
            "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|       3464.917632|    0|          6.0|      2.0|\n",
            "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|508.55270400000006|    1|          0.0|      0.0|\n",
            "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|        542.348928|    0|          0.0|      1.0|\n",
            "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|       1989.149184|    0|          1.0|      0.0|\n",
            "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|        415.210752|    0|          1.0|      0.0|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+-----+-------------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "We can check that the index of zero has the most observations.\n",
            "Carrier 'UA' has the index 0 and have observations =  13170\n",
            "Carrier 'US' has the index 6 and have observations =  2740\n",
            "+-------------+---------+--------+----------------------------------------------------+\n",
            "|carrier_index|org_index|duration|features                                            |\n",
            "+-------------+---------+--------+----------------------------------------------------+\n",
            "|6.0          |2.0      |351     |[11.0,20.0,6.0,6.0,2.0,3464.917632,9.48,351.0]      |\n",
            "|0.0          |0.0      |82      |[0.0,22.0,2.0,0.0,0.0,508.55270400000006,16.33,82.0]|\n",
            "|0.0          |1.0      |82      |[2.0,20.0,4.0,0.0,1.0,542.348928,6.17,82.0]         |\n",
            "|1.0          |0.0      |195     |[9.0,13.0,1.0,1.0,0.0,1989.149184,10.33,195.0]      |\n",
            "|1.0          |0.0      |65      |[4.0,2.0,5.0,1.0,0.0,415.210752,8.92,65.0]          |\n",
            "+-------------+---------+--------+----------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xIkUFLUYzwe",
        "outputId": "f14a9bf3-7f91-47ca-9a97-9e857fd78861"
      },
      "source": [
        "from pyspark.sql.functions import when\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\r\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\r\n",
        "\r\n",
        "df = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "df = df.withColumn(\"kilometre\", df[\"mile\"]*1.609344)\r\n",
        "df = df.withColumn(\"label\", when(df[\"delay\"] >= 15, 1).otherwise(0))\r\n",
        "df = StringIndexer(inputCol=\"carrier\",  outputCol=\"carrier_index\").fit(df).transform(df)\r\n",
        "df = StringIndexer(inputCol=\"org\", outputCol=\"org_index\").fit(df).transform(df)\r\n",
        "df = VectorAssembler(inputCols=[\"mon\", \"dom\", \"dow\", \"carrier_index\", \"org_index\", \"kilometre\", \"depart\", \"duration\"], outputCol='features').transform(df)\r\n",
        "df.show(5, truncate=False)\r\n",
        "\r\n",
        "# Specify the seed for reproducability\r\n",
        "df_train, df_test = df.randomSplit([0.8, 0.2], seed=42)\r\n",
        "\r\n",
        "# Build a Decision Tree\r\n",
        "tree = DecisionTreeClassifier()\r\n",
        "\r\n",
        "# Train the data\r\n",
        "tree_model = tree.fit(df_train)\r\n",
        "\r\n",
        "# Evaluating the predictions from the test data and compare to known values\r\n",
        "prediction = tree_model.transform(df_test)\r\n",
        "prediction[[\"label\", \"prediction\", \"probability\"]].show(5, truncate=False)\r\n",
        "\r\n",
        "# Confusion matrix\r\n",
        "prediction.groupBy(\"label\", \"prediction\").count().show()\r\n",
        "TP = prediction.where(\"label = 1 AND prediction = 1\").count()\r\n",
        "TN = prediction.where(\"label = 0 AND prediction = 0\").count()\r\n",
        "FP = prediction.where(\"label = 0 AND prediction = 1\").count()\r\n",
        "FN = prediction.where(\"label = 1 AND prediction = 0\").count()\r\n",
        "print(f\"The accuracy is {(TP+TN)/(TP+TN+FP+FN)*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+-----+-------------+---------+----------------------------------------------------+\n",
            "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|kilometre         |label|carrier_index|org_index|features                                            |\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+-----+-------------+---------+----------------------------------------------------+\n",
            "|11 |20 |6  |US     |19    |JFK|2153|9.48  |351     |null |3464.917632       |0    |6.0          |2.0      |[11.0,20.0,6.0,6.0,2.0,3464.917632,9.48,351.0]      |\n",
            "|0  |22 |2  |UA     |1107  |ORD|316 |16.33 |82      |30   |508.55270400000006|1    |0.0          |0.0      |[0.0,22.0,2.0,0.0,0.0,508.55270400000006,16.33,82.0]|\n",
            "|2  |20 |4  |UA     |226   |SFO|337 |6.17  |82      |-8   |542.348928        |0    |0.0          |1.0      |[2.0,20.0,4.0,0.0,1.0,542.348928,6.17,82.0]         |\n",
            "|9  |13 |1  |AA     |419   |ORD|1236|10.33 |195     |-5   |1989.149184       |0    |1.0          |0.0      |[9.0,13.0,1.0,1.0,0.0,1989.149184,10.33,195.0]      |\n",
            "|4  |2  |5  |AA     |325   |ORD|258 |8.92  |65      |null |415.210752        |0    |1.0          |0.0      |[4.0,2.0,5.0,1.0,0.0,415.210752,8.92,65.0]          |\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+-----+-------------+---------+----------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----+----------+----------------------------------------+\n",
            "|label|prediction|probability                             |\n",
            "+-----+----------+----------------------------------------+\n",
            "|1    |0.0       |[0.5577854671280277,0.4422145328719723] |\n",
            "|1    |1.0       |[0.4491869918699187,0.5508130081300813] |\n",
            "|0    |1.0       |[0.4392470852647844,0.5607529147352156] |\n",
            "|1    |1.0       |[0.35910253429435013,0.6408974657056499]|\n",
            "|1    |1.0       |[0.35910253429435013,0.6408974657056499]|\n",
            "+-----+----------+----------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 1436|\n",
            "|    0|       0.0| 2835|\n",
            "|    1|       1.0| 3401|\n",
            "|    0|       1.0| 2380|\n",
            "+-----+----------+-----+\n",
            "\n",
            "The accuracy is 62.04%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPsdT2FWb4Rj",
        "outputId": "c454432e-dc53-404c-a1e2-cd65053a72a1"
      },
      "source": [
        "# Logistic Regression\r\n",
        "from pyspark.ml.classification import LogisticRegression\r\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\r\n",
        "df = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "df = df.withColumn(\"kilometre\", df[\"mile\"]*1.609344)\r\n",
        "df = df.withColumn(\"label\", when(df[\"delay\"] >= 15, 1).otherwise(0))\r\n",
        "df = StringIndexer(inputCol=\"carrier\",  outputCol=\"carrier_index\").fit(df).transform(df)\r\n",
        "df = StringIndexer(inputCol=\"org\", outputCol=\"org_index\").fit(df).transform(df)\r\n",
        "df = VectorAssembler(inputCols=[\"mon\", \"dom\", \"dow\", \"carrier_index\", \"org_index\", \"kilometre\", \"depart\", \"duration\"], outputCol='features').transform(df)\r\n",
        "df.show(5, truncate=False)\r\n",
        "\r\n",
        "# Specify the seed for reproducability\r\n",
        "df_train, df_test = df.randomSplit([0.8, 0.2], seed=42)\r\n",
        "\r\n",
        "# Create a logistic regression classifier\r\n",
        "logistic = LogisticRegression()\r\n",
        "\r\n",
        "# Train the data\r\n",
        "logistic = logistic.fit(df_train) \r\n",
        "prediction = logistic.transform(df_test)\r\n",
        "prediction[[\"label\", \"prediction\", \"probability\"]].show(5, truncate=False)\r\n",
        "\r\n",
        "# Confusion matrix\r\n",
        "prediction.groupBy(\"label\", \"prediction\").count().show()\r\n",
        "TP = prediction.where(\"label = 1 AND prediction = 1\").count()\r\n",
        "TN = prediction.where(\"label = 0 AND prediction = 0\").count()\r\n",
        "FP = prediction.where(\"label = 0 AND prediction = 1\").count()\r\n",
        "FN = prediction.where(\"label = 1 AND prediction = 0\").count()\r\n",
        "print(f\"The accuracy is {(TP+TN)/(TP+TN+FP+FN)*100:.2f}%\")\r\n",
        "\r\n",
        "# Precision is the proportion of positive predictions which are correct. For all flights which are predicted to be delayed, what proportion is actually delayed?\r\n",
        "print(f\"The precision is {TP/(TP+FP)*100:.2f}%\")\r\n",
        "\r\n",
        "# Recall is the proportion of positives outcomes which are correctly predicted. For all delayed flights, what proportion is correctly predicted by the model?\r\n",
        "print(f\"The recall is {TP/(TP+FN)*100:.2f}%\")\r\n",
        "\r\n",
        "# Weighted metrics\r\n",
        "evaluator = MulticlassClassificationEvaluator()\r\n",
        "print(evaluator.evaluate(prediction, {evaluator.metricName: \"weightedPrecision\"}))\r\n",
        "binary_evaluator = BinaryClassificationEvaluator()\r\n",
        "print(binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+-----+-------------+---------+----------------------------------------------------+\n",
            "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|kilometre         |label|carrier_index|org_index|features                                            |\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+-----+-------------+---------+----------------------------------------------------+\n",
            "|11 |20 |6  |US     |19    |JFK|2153|9.48  |351     |null |3464.917632       |0    |6.0          |2.0      |[11.0,20.0,6.0,6.0,2.0,3464.917632,9.48,351.0]      |\n",
            "|0  |22 |2  |UA     |1107  |ORD|316 |16.33 |82      |30   |508.55270400000006|1    |0.0          |0.0      |[0.0,22.0,2.0,0.0,0.0,508.55270400000006,16.33,82.0]|\n",
            "|2  |20 |4  |UA     |226   |SFO|337 |6.17  |82      |-8   |542.348928        |0    |0.0          |1.0      |[2.0,20.0,4.0,0.0,1.0,542.348928,6.17,82.0]         |\n",
            "|9  |13 |1  |AA     |419   |ORD|1236|10.33 |195     |-5   |1989.149184       |0    |1.0          |0.0      |[9.0,13.0,1.0,1.0,0.0,1989.149184,10.33,195.0]      |\n",
            "|4  |2  |5  |AA     |325   |ORD|258 |8.92  |65      |null |415.210752        |0    |1.0          |0.0      |[4.0,2.0,5.0,1.0,0.0,415.210752,8.92,65.0]          |\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+-----+-------------+---------+----------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----+----------+----------------------------------------+\n",
            "|label|prediction|probability                             |\n",
            "+-----+----------+----------------------------------------+\n",
            "|1    |1.0       |[0.3371934771949116,0.6628065228050884] |\n",
            "|1    |0.0       |[0.5733817375216156,0.4266182624783844] |\n",
            "|0    |1.0       |[0.4270099319219388,0.5729900680780612] |\n",
            "|1    |1.0       |[0.3677192196053865,0.6322807803946136] |\n",
            "|1    |1.0       |[0.42884456238628743,0.5711554376137126]|\n",
            "+-----+----------+----------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 2185|\n",
            "|    0|       0.0| 3300|\n",
            "|    1|       1.0| 2652|\n",
            "|    0|       1.0| 1915|\n",
            "+-----+----------+-----+\n",
            "\n",
            "The accuracy is 59.21%\n",
            "The precision is 58.07%\n",
            "The recall is 54.83%\n",
            "0.5915581585405116\n",
            "0.6353273771945287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubEuJyqQyzxc",
        "outputId": "323aa1c6-383c-442d-e91e-b5466b323f14"
      },
      "source": [
        "### Turning text into tables\r\n",
        "from pyspark.sql.functions import regexp_replace\r\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\r\n",
        "\r\n",
        "# Removing punctuation\r\n",
        "REGEX = '[_():;,.!?\\\\- 0-9]'\r\n",
        "\r\n",
        "sms = spark.read.csv(\"sms.csv\", inferSchema=True, sep=\";\")\r\n",
        "sms.show(5, truncate=False)\r\n",
        "\r\n",
        "sms = sms.withColumn(\"text\", regexp_replace(\"_c1\", REGEX, ' '))\r\n",
        "sms[[\"_c1\", \"text\"]].show(5, truncate=False)\r\n",
        "\r\n",
        "# Text to tokens\r\n",
        "sms = Tokenizer(inputCol=\"text\", outputCol=\"tokens\").transform(sms)\r\n",
        "sms[[\"text\", \"tokens\"]].show(5, truncate=False)\r\n",
        "\r\n",
        "# Remove stop words\r\n",
        "stopwords = StopWordsRemover()\r\n",
        "\r\n",
        "# What are stop words?\r\n",
        "print(stopwords.getStopWords())\r\n",
        "\r\n",
        "# Specify the input and output column names  inputCol ต้องเป็น list จาก tokenization\r\n",
        "stopwords = stopwords.setInputCol(\"tokens\").setOutputCol(\"words\")\r\n",
        "sms = stopwords.transform(sms)\r\n",
        "sms[[\"text\", \"words\"]].show(5, truncate=False)\r\n",
        "\r\n",
        "# Hasing words, numFeatures คือ จำนวนสูงสุดของ element ใน sparse vector ที่สสร้างด้วย HashingTF\r\n",
        "# [4, win, 1000, cash, prize, prize, worth, 5000, 1]|(32,[3,5,11,15,18,26,30,31],[1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0])\r\n",
        "# คำว่า prize มี hash value = 18 เกิดขึ้น 2 ครั้ง ?\r\n",
        "hasher = HashingTF(inputCol=\"words\", outputCol=\"hash\", numFeatures=32)\r\n",
        "sms = hasher.transform(sms)\r\n",
        "sms[[\"words\", \"hash\"]].show(5, truncate=False)\r\n",
        "\r\n",
        "# Dealing with common words\r\n",
        "# คำที่โผล่มาบ่อย จะให้ค่าน้ำหนักน้อยลง คำที่โผล่มาน้อยจะให้ค่าน้ำหนักเพิ่ม\r\n",
        "sms = IDF(inputCol=\"hash\", outputCol=\"features\").fit(sms).transform(sms)\r\n",
        "sms[[\"features\"]].show(5, truncate=False)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---------------------------------------------------------------------------------------------------------------+---+\n",
            "|_c0|_c1                                                                                                            |_c2|\n",
            "+---+---------------------------------------------------------------------------------------------------------------+---+\n",
            "|1  |Sorry, I'll call later in meeting                                                                              |0  |\n",
            "|2  |Dont worry. I guess he's busy.                                                                                 |0  |\n",
            "|3  |Call FREEPHONE 0800 542 0578 now!                                                                              |1  |\n",
            "|4  |Win a 1000 cash prize or a prize worth 5000                                                                    |1  |\n",
            "|5  |Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...|0  |\n",
            "+---+---------------------------------------------------------------------------------------------------------------+---+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
            "|_c1                                                                                                            |text                                                                                                           |\n",
            "+---------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
            "|Sorry, I'll call later in meeting                                                                              |Sorry  I'll call later in meeting                                                                              |\n",
            "|Dont worry. I guess he's busy.                                                                                 |Dont worry  I guess he's busy                                                                                  |\n",
            "|Call FREEPHONE 0800 542 0578 now!                                                                              |Call FREEPHONE               now                                                                               |\n",
            "|Win a 1000 cash prize or a prize worth 5000                                                                    |Win a      cash prize or a prize worth                                                                         |\n",
            "|Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...|Go until jurong point  crazy   Available only in bugis n great world la e buffet    Cine there got amore wat   |\n",
            "+---------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                                                           |tokens                                                                                                                                 |\n",
            "+---------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Sorry  I'll call later in meeting                                                                              |[sorry, , i'll, call, later, in, meeting]                                                                                              |\n",
            "|Dont worry  I guess he's busy                                                                                  |[dont, worry, , i, guess, he's, busy]                                                                                                  |\n",
            "|Call FREEPHONE               now                                                                               |[call, freephone, , , , , , , , , , , , , , , now]                                                                                     |\n",
            "|Win a      cash prize or a prize worth                                                                         |[win, a, , , , , , cash, prize, or, a, prize, worth]                                                                                   |\n",
            "|Go until jurong point  crazy   Available only in bugis n great world la e buffet    Cine there got amore wat   |[go, until, jurong, point, , crazy, , , available, only, in, bugis, n, great, world, la, e, buffet, , , , cine, there, got, amore, wat]|\n",
            "+---------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\", \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\", \"i'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\", \"i've\", \"we've\", \"you've\", \"they've\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"mustn't\", \"can't\", \"couldn't\", 'cannot', 'could', \"here's\", \"how's\", \"let's\", 'ought', \"that's\", \"there's\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", 'would']\n",
            "+---------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                                                           |words                                                                                                          |\n",
            "+---------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
            "|Sorry  I'll call later in meeting                                                                              |[sorry, , call, later, meeting]                                                                                |\n",
            "|Dont worry  I guess he's busy                                                                                  |[dont, worry, , guess, busy]                                                                                   |\n",
            "|Call FREEPHONE               now                                                                               |[call, freephone, , , , , , , , , , , , , , ]                                                                  |\n",
            "|Win a      cash prize or a prize worth                                                                         |[win, , , , , , cash, prize, prize, worth]                                                                     |\n",
            "|Go until jurong point  crazy   Available only in bugis n great world la e buffet    Cine there got amore wat   |[go, jurong, point, , crazy, , , available, bugis, n, great, world, la, e, buffet, , , , cine, got, amore, wat]|\n",
            "+---------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+\n",
            "|words                                                                                                          |hash                                                                                     |\n",
            "+---------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+\n",
            "|[sorry, , call, later, meeting]                                                                                |(32,[0,1,4,10,28],[1.0,1.0,1.0,1.0,1.0])                                                 |\n",
            "|[dont, worry, , guess, busy]                                                                                   |(32,[9,20,23,28],[2.0,1.0,1.0,1.0])                                                      |\n",
            "|[call, freephone, , , , , , , , , , , , , , ]                                                                  |(32,[5,10,28],[1.0,1.0,14.0])                                                            |\n",
            "|[win, , , , , , cash, prize, prize, worth]                                                                     |(32,[5,15,28,30,31],[2.0,1.0,5.0,1.0,1.0])                                               |\n",
            "|[go, jurong, point, , crazy, , , available, bugis, n, great, world, la, e, buffet, , , , cine, got, amore, wat]|(32,[0,3,4,10,11,12,14,16,26,27,28,31],[2.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,1.0,2.0,6.0,1.0])|\n",
            "+---------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|features                                                                                                                                                                                                                                                               |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|(32,[0,1,4,10,28],[1.4459777220255234,1.535137774248336,1.6809965326024856,1.1695930411521103,0.26084076190996386])                                                                                                                                                    |\n",
            "|(32,[9,20,23,28],[2.9727745207268,1.6469023212595095,1.5647132294178818,0.26084076190996386])                                                                                                                                                                          |\n",
            "|(32,[5,10,28],[1.3662279859651334,1.1695930411521103,3.651770666739494])                                                                                                                                                                                               |\n",
            "|(32,[5,15,28,30,31],[2.732455971930267,1.5698723122279092,1.3042038095498194,1.7415609442855369,1.5898991025777833])                                                                                                                                                   |\n",
            "|(32,[0,3,4,10,11,12,14,16,26,27,28,31],[2.891955444051047,1.1695930411521103,1.6809965326024856,1.1695930411521103,3.0503913747685387,1.728342653199684,2.5756188925254606,2.728241007251137,1.4808514613311483,4.0146172256222,1.5650445714597832,1.5898991025777833])|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvf-L7OkKmVG",
        "outputId": "c773b1d5-4a91-426f-95db-7eea5961c272"
      },
      "source": [
        "# Example from exercise\r\n",
        "from pyspark.sql.functions import regexp_replace\r\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\r\n",
        "from pyspark.ml.classification import LogisticRegression\r\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\r\n",
        "\r\n",
        "REGEX = '[_():;,.!?\\\\- 0-9]'\r\n",
        "\r\n",
        "sms = spark.read.csv(\"sms.csv\", inferSchema=True, sep=\";\")\r\n",
        "for i, j in zip([\"_c0\", \"_c1\", \"_c2\"], [\"id\", \"text\", \"label\"]):\r\n",
        "    sms = sms.withColumnRenamed(i, j)\r\n",
        "sms.show(5)\r\n",
        "\r\n",
        "# Remove punctuation (REGEX provided) and numbers\r\n",
        "sms = sms.withColumn('text', regexp_replace(\"text\", '[_():;,.!?\\\\- 0-9]', ' '))\r\n",
        "\r\n",
        "# Merge multiple spaces\r\n",
        "sms = sms.withColumn('text', regexp_replace(\"text\", ' +', ' '))\r\n",
        "\r\n",
        "# Split the text into words\r\n",
        "sms = Tokenizer(inputCol='text', outputCol=\"words\").transform(sms)\r\n",
        "sms.show(4, truncate=False)\r\n",
        "\r\n",
        "# Remove stop words.\r\n",
        "sms = StopWordsRemover(inputCol=\"words\", outputCol=\"terms\").transform(sms)\r\n",
        "\r\n",
        "# Apply the hashing trick\r\n",
        "sms = HashingTF(inputCol=\"terms\", outputCol=\"hash\", numFeatures=1024).transform(sms)\r\n",
        "\r\n",
        "# Convert hashed symbols to TF-IDF\r\n",
        "sms = IDF(inputCol=\"hash\", outputCol=\"features\").fit(sms).transform(sms)\r\n",
        "      \r\n",
        "sms.select('terms', 'features').show(4, truncate=False)\r\n",
        "\r\n",
        "# Split the data into training and testing sets\r\n",
        "sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed=13)\r\n",
        "\r\n",
        "# Fit a Logistic Regression model to the training data\r\n",
        "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\r\n",
        "\r\n",
        "# Make predictions on the testing data\r\n",
        "prediction = logistic.transform(sms_test)\r\n",
        "\r\n",
        "# Create a confusion matrix, comparing predictions to known labels\r\n",
        "prediction.groupBy(\"label\", \"prediction\").count().show()\r\n",
        "\r\n",
        "TP = prediction.where(\"label = 1 AND prediction = 1\").count()\r\n",
        "TN = prediction.where(\"label = 0 AND prediction = 0\").count()\r\n",
        "FP = prediction.where(\"label = 0 AND prediction = 1\").count()\r\n",
        "FN = prediction.where(\"label = 1 AND prediction = 0\").count()\r\n",
        "print(f\"The accuracy is {(TP+TN)/(TP+TN+FP+FN)*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+-----+\n",
            "| id|                text|label|\n",
            "+---+--------------------+-----+\n",
            "|  1|Sorry, I'll call ...|    0|\n",
            "|  2|Dont worry. I gue...|    0|\n",
            "|  3|Call FREEPHONE 08...|    1|\n",
            "|  4|Win a 1000 cash p...|    1|\n",
            "|  5|Go until jurong p...|    0|\n",
            "+---+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+----------------------------------+-----+------------------------------------------+\n",
            "|id |text                              |label|words                                     |\n",
            "+---+----------------------------------+-----+------------------------------------------+\n",
            "|1  |Sorry I'll call later in meeting  |0    |[sorry, i'll, call, later, in, meeting]   |\n",
            "|2  |Dont worry I guess he's busy      |0    |[dont, worry, i, guess, he's, busy]       |\n",
            "|3  |Call FREEPHONE now                |1    |[call, freephone, now]                    |\n",
            "|4  |Win a cash prize or a prize worth |1    |[win, a, cash, prize, or, a, prize, worth]|\n",
            "+---+----------------------------------+-----+------------------------------------------+\n",
            "only showing top 4 rows\n",
            "\n",
            "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|terms                           |features                                                                                            |\n",
            "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|[sorry, call, later, meeting]   |(1024,[138,384,577,996],[2.273418200008753,3.6288353225642043,3.5890949939146903,4.104259019279279])|\n",
            "|[dont, worry, guess, busy]      |(1024,[215,233,276,329],[3.9913186080986836,3.3790235241678332,4.734227298217693,4.58299632849377]) |\n",
            "|[call, freephone]               |(1024,[133,138],[5.367951058306837,2.273418200008753])                                              |\n",
            "|[win, cash, prize, prize, worth]|(1024,[31,47,62,389],[3.6632029660684124,4.754846585420428,4.072170704727778,7.064594791043114])    |\n",
            "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 4 rows\n",
            "\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0|   41|\n",
            "|    0|       0.0|  948|\n",
            "|    1|       1.0|  105|\n",
            "|    0|       1.0|    2|\n",
            "+-----+----------+-----+\n",
            "\n",
            "The accuracy is 96.08%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5Y62MRuToj6",
        "outputId": "195e5564-bf8e-416d-c913-425e0a19d3e7"
      },
      "source": [
        "# Onehot Encoder\r\n",
        "\r\n",
        "from pyspark.sql.functions import when\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\r\n",
        "\r\n",
        "df = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "df = df.withColumn(\"kilometre\", df[\"mile\"]*1.609344)\r\n",
        "df = df.withColumn(\"label\", when(df[\"delay\"] >= 15, 1).otherwise(0))\r\n",
        "df = StringIndexer(inputCol=\"carrier\",  outputCol=\"carrier_index\").fit(df).transform(df)\r\n",
        "df = StringIndexer(inputCol=\"org\", outputCol=\"org_index\").fit(df).transform(df)\r\n",
        "df.show(5, truncate=False)\r\n",
        "\r\n",
        "# Create onehot encoder\r\n",
        "onehot = OneHotEncoder(inputCol=\"carrier_index\", outputCol=\"carrier_dummy\")\r\n",
        "onehot.fit(df).transform(df)[[\"carrier\", \"carrier_index\", \"carrier_dummy\"]].distinct().sort(\"carrier_index\").show(truncate=False)\r\n",
        "\r\n",
        "onehots = OneHotEncoder(inputCols=[\"carrier_index\", \"org_index\"], outputCols=[\"carrier_dummy\", \"org_dummy\"])\r\n",
        "onehots.fit(df).transform(df)[[\"carrier\", \"carrier_index\", \"carrier_dummy\", \"org\", \"org_index\", \"org_dummy\"]].distinct().sort(\"org_index\").show(truncate=False)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+-----+-------------+---------+\n",
            "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|kilometre         |label|carrier_index|org_index|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+-----+-------------+---------+\n",
            "|11 |20 |6  |US     |19    |JFK|2153|9.48  |351     |null |3464.917632       |0    |6.0          |2.0      |\n",
            "|0  |22 |2  |UA     |1107  |ORD|316 |16.33 |82      |30   |508.55270400000006|1    |0.0          |0.0      |\n",
            "|2  |20 |4  |UA     |226   |SFO|337 |6.17  |82      |-8   |542.348928        |0    |0.0          |1.0      |\n",
            "|9  |13 |1  |AA     |419   |ORD|1236|10.33 |195     |-5   |1989.149184       |0    |1.0          |0.0      |\n",
            "|4  |2  |5  |AA     |325   |ORD|258 |8.92  |65      |null |415.210752        |0    |1.0          |0.0      |\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+------------------+-----+-------------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+-------------+-------------+\n",
            "|carrier|carrier_index|carrier_dummy|\n",
            "+-------+-------------+-------------+\n",
            "|UA     |0.0          |(8,[0],[1.0])|\n",
            "|AA     |1.0          |(8,[1],[1.0])|\n",
            "|OO     |2.0          |(8,[2],[1.0])|\n",
            "|WN     |3.0          |(8,[3],[1.0])|\n",
            "|B6     |4.0          |(8,[4],[1.0])|\n",
            "|OH     |5.0          |(8,[5],[1.0])|\n",
            "|US     |6.0          |(8,[6],[1.0])|\n",
            "|HA     |7.0          |(8,[7],[1.0])|\n",
            "|AQ     |8.0          |(8,[],[])    |\n",
            "+-------+-------------+-------------+\n",
            "\n",
            "+-------+-------------+-------------+---+---------+-------------+\n",
            "|carrier|carrier_index|carrier_dummy|org|org_index|org_dummy    |\n",
            "+-------+-------------+-------------+---+---------+-------------+\n",
            "|B6     |4.0          |(8,[4],[1.0])|ORD|0.0      |(7,[0],[1.0])|\n",
            "|US     |6.0          |(8,[6],[1.0])|ORD|0.0      |(7,[0],[1.0])|\n",
            "|AA     |1.0          |(8,[1],[1.0])|ORD|0.0      |(7,[0],[1.0])|\n",
            "|OH     |5.0          |(8,[5],[1.0])|ORD|0.0      |(7,[0],[1.0])|\n",
            "|OO     |2.0          |(8,[2],[1.0])|ORD|0.0      |(7,[0],[1.0])|\n",
            "|UA     |0.0          |(8,[0],[1.0])|ORD|0.0      |(7,[0],[1.0])|\n",
            "|B6     |4.0          |(8,[4],[1.0])|SFO|1.0      |(7,[1],[1.0])|\n",
            "|OO     |2.0          |(8,[2],[1.0])|SFO|1.0      |(7,[1],[1.0])|\n",
            "|HA     |7.0          |(8,[7],[1.0])|SFO|1.0      |(7,[1],[1.0])|\n",
            "|AA     |1.0          |(8,[1],[1.0])|SFO|1.0      |(7,[1],[1.0])|\n",
            "|UA     |0.0          |(8,[0],[1.0])|SFO|1.0      |(7,[1],[1.0])|\n",
            "|US     |6.0          |(8,[6],[1.0])|SFO|1.0      |(7,[1],[1.0])|\n",
            "|WN     |3.0          |(8,[3],[1.0])|SFO|1.0      |(7,[1],[1.0])|\n",
            "|OH     |5.0          |(8,[5],[1.0])|JFK|2.0      |(7,[2],[1.0])|\n",
            "|B6     |4.0          |(8,[4],[1.0])|JFK|2.0      |(7,[2],[1.0])|\n",
            "|UA     |0.0          |(8,[0],[1.0])|JFK|2.0      |(7,[2],[1.0])|\n",
            "|US     |6.0          |(8,[6],[1.0])|JFK|2.0      |(7,[2],[1.0])|\n",
            "|AA     |1.0          |(8,[1],[1.0])|JFK|2.0      |(7,[2],[1.0])|\n",
            "|B6     |4.0          |(8,[4],[1.0])|LGA|3.0      |(7,[3],[1.0])|\n",
            "|UA     |0.0          |(8,[0],[1.0])|LGA|3.0      |(7,[3],[1.0])|\n",
            "+-------+-------------+-------------+---+---------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOIKlZj33Zos",
        "outputId": "c898db28-7bc9-4e02-8642-3bb995c076da"
      },
      "source": [
        "# Linear regression\r\n",
        "from pyspark.ml.regression import LinearRegression\r\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
        "from pyspark.sql.functions import when\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\r\n",
        "\r\n",
        "df = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "df = df.withColumn(\"km\", df[\"mile\"]*1.609344).drop(\"mile\")\r\n",
        "\r\n",
        "# One variable regression: Y = duration, X = km\r\n",
        "df = VectorAssembler(inputCols=[\"km\"], outputCol='features').transform(df)\r\n",
        "regression = LinearRegression(labelCol=\"duration\")\r\n",
        "df_train, df_test = df.randomSplit([0.8, 0.2], seed=42)\r\n",
        "regression = regression.fit(df_train)\r\n",
        "predictions = regression.transform(df_test)\r\n",
        "print(\"RMSE = \", RegressionEvaluator(labelCol=\"duration\").evaluate(predictions))\r\n",
        "predictions[[\"duration\", \"prediction\"]].show(5)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE =  17.098406021968962\n",
            "+--------+------------------+\n",
            "|duration|        prediction|\n",
            "+--------+------------------+\n",
            "|     560| 560.7390896231818|\n",
            "|     310| 346.9189834661732|\n",
            "|      90|  85.0289046671506|\n",
            "|     130|133.58566184224821|\n",
            "|     251|245.42440831822486|\n",
            "+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N8o6JgwJzhP",
        "outputId": "116c49d8-04f1-48e0-af20-7c47405c04c5"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\r\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\r\n",
        "\r\n",
        "df = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "df = df.withColumn(\"km\", df[\"mile\"]*1.609344).drop(\"mile\")\r\n",
        "df = StringIndexer(inputCol=\"carrier\",  outputCol=\"carrier_index\").fit(df).transform(df)\r\n",
        "df = StringIndexer(inputCol=\"org\", outputCol=\"org_index\").fit(df).transform(df)\r\n",
        "onehots = OneHotEncoder(inputCols=[\"carrier_index\", \"org_index\"], outputCols=[\"carrier_dummy\", \"org_dummy\"])\r\n",
        "df = onehots.fit(df).transform(df)\r\n",
        "\r\n",
        "# Including only the distance of the flight (the km column) as a predictor.\r\n",
        "df = VectorAssembler(inputCols=[\"km\"], outputCol='features').transform(df)\r\n",
        "df_train, df_test = df.randomSplit([0.8, 0.2], seed=42)\r\n",
        "\r\n",
        "# Create a regression object and train on training data\r\n",
        "regression = LinearRegression(labelCol=\"duration\").fit(df_train)\r\n",
        "\r\n",
        "# Create predictions for the testing data and take a look at the predictions\r\n",
        "predictions = regression.transform(df_test)\r\n",
        "predictions.select('duration', 'prediction').show(5, False)\r\n",
        "\r\n",
        "# Calculate the RMSE\r\n",
        "print(f'RMSE={RegressionEvaluator(labelCol=\"duration\").evaluate(predictions)}')\r\n",
        "predictions[[\"duration\", \"prediction\"]].show(5)\r\n",
        "\r\n",
        "# Intercept (average minutes on ground)\r\n",
        "inter = regression.intercept\r\n",
        "print(\"intercept=\",inter)\r\n",
        "\r\n",
        "# Coefficients\r\n",
        "coefs = regression.coefficients\r\n",
        "print(coefs)\r\n",
        "\r\n",
        "# Average minutes per km\r\n",
        "minutes_per_km = regression.coefficients[0]\r\n",
        "print(minutes_per_km)\r\n",
        "\r\n",
        "# Average speed in km per hour\r\n",
        "avg_speed = 60 / minutes_per_km \r\n",
        "print(avg_speed)\r\n",
        "print(\"p-values\")\r\n",
        "print(regression.summary.pValues)\r\n",
        "print(\"std errors\")\r\n",
        "print(regression.summary.coefficientStandardErrors)\r\n",
        "print(\"t-statistic\")\r\n",
        "print(regression.intercept/regression.summary.coefficientStandardErrors[0], regression.coefficients[0]/regression.summary.coefficientStandardErrors[0])\r\n",
        "\r\n",
        "# T-statistic of estimated coefficients and intercept.\r\n",
        "print(regression.summary.tValues)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+------------------+\n",
            "|duration|prediction        |\n",
            "+--------+------------------+\n",
            "|560     |560.7390896231818 |\n",
            "|310     |346.9189834661732 |\n",
            "|90      |85.0289046671506  |\n",
            "|130     |133.58566184224821|\n",
            "|251     |245.42440831822486|\n",
            "+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "RMSE=17.098406021968962\n",
            "+--------+------------------+\n",
            "|duration|        prediction|\n",
            "+--------+------------------+\n",
            "|     560| 560.7390896231818|\n",
            "|     310| 346.9189834661732|\n",
            "|      90|  85.0289046671506|\n",
            "|     130|133.58566184224821|\n",
            "|     251|245.42440831822486|\n",
            "+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "intercept= 44.38239615466289\n",
            "[0.07561847142122584]\n",
            "0.07561847142122584\n",
            "793.4569275511461\n",
            "p-values\n",
            "[0.0, 0.0]\n",
            "std errors\n",
            "[7.601309964235278e-05, 0.1378246804145554]\n",
            "t-statistic\n",
            "583878.257346238 994.808418246543\n",
            "[994.808418246543, 322.0206716327401]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX96xcUCLDX9",
        "outputId": "d348db4d-9e19-4029-a0e2-51d97cdf9def"
      },
      "source": [
        "# Linear regression: including more independent variables\r\n",
        "\r\n",
        "from pyspark.ml.regression import LinearRegression\r\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\r\n",
        "\r\n",
        "df = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "df = df.withColumn(\"km\", df[\"mile\"]*1.609344).drop(\"mile\")\r\n",
        "df = StringIndexer(inputCol=\"org\", outputCol=\"org_index\").fit(df).transform(df)\r\n",
        "onehots = OneHotEncoder(inputCols=[\"org_index\"], outputCols=[\"org_dummy\"])\r\n",
        "df = onehots.fit(df).transform(df)\r\n",
        "\r\n",
        "# Including both distance and airports as features.\r\n",
        "df = VectorAssembler(inputCols=[\"km\", \"org_dummy\"], outputCol='features').transform(df)\r\n",
        "df[[\"km\", \"org_index\", \"org_dummy\", \"features\"]].show(5, truncate=False)\r\n",
        "df_train, df_test = df.randomSplit([0.8, 0.2], seed=42)\r\n",
        "\r\n",
        "# Create a regression object and train on training data\r\n",
        "regression = LinearRegression(labelCol=\"duration\").fit(df_train)\r\n",
        "\r\n",
        "# Create predictions for the testing data\r\n",
        "predictions = regression.transform(df_test)\r\n",
        "\r\n",
        "# Calculate the RMSE on testing data\r\n",
        "print(\"RMSE = \", RegressionEvaluator(labelCol=\"duration\").evaluate(predictions))\r\n",
        "predictions[[\"duration\", \"prediction\"]].show(5, truncate=False)\r\n",
        "df[[\"org\", \"org_dummy\"]].distinct().sort(\"org_index\").show(truncate=False)\r\n",
        "# Average speed in km per hour\r\n",
        "avg_speed_hour = 60 / regression.coefficients[0]\r\n",
        "print(avg_speed_hour)\r\n",
        "\r\n",
        "# Average minutes on ground at OGG\r\n",
        "inter = regression.intercept\r\n",
        "print(\"intercept=\", inter, \"which is the average minutes on ground at OGG airport.\")\r\n",
        "\r\n",
        "# Average minutes on ground at JFK\r\n",
        "avg_ground_jfk = inter + regression.coefficients[3]\r\n",
        "print(avg_ground_jfk)\r\n",
        "\r\n",
        "# Average minutes on ground at LGA\r\n",
        "avg_ground_lga = inter + regression.coefficients[4]\r\n",
        "print(avg_ground_lga, \"\\n\")\r\n",
        "\r\n",
        "# regression.coefficients[0] เป็น slope ของ ตัวแปร km\r\n",
        "print(\"coefficients = \", regression.coefficients, \"has \", len(regression.coefficients), \"slopes\")\r\n",
        "\r\n",
        "# p-values (including intercepts)\r\n",
        "print(\"p-values =\", regression.summary.pValues)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+---------+-------------+----------------------------------+\n",
            "|km                |org_index|org_dummy    |features                          |\n",
            "+------------------+---------+-------------+----------------------------------+\n",
            "|3464.917632       |2.0      |(7,[2],[1.0])|(8,[0,3],[3464.917632,1.0])       |\n",
            "|508.55270400000006|0.0      |(7,[0],[1.0])|(8,[0,1],[508.55270400000006,1.0])|\n",
            "|542.348928        |1.0      |(7,[1],[1.0])|(8,[0,2],[542.348928,1.0])        |\n",
            "|1989.149184       |0.0      |(7,[0],[1.0])|(8,[0,1],[1989.149184,1.0])       |\n",
            "|415.210752        |0.0      |(7,[0],[1.0])|(8,[0,1],[415.210752,1.0])        |\n",
            "+------------------+---------+-------------+----------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "RMSE =  11.068097345225052\n",
            "+--------+------------------+\n",
            "|duration|prediction        |\n",
            "+--------+------------------+\n",
            "|560     |551.6335945083583 |\n",
            "|310     |313.1047029694895 |\n",
            "|90      |84.2679368060932  |\n",
            "|130     |131.97295021699213|\n",
            "|251     |241.84991093031567|\n",
            "+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+-------------+\n",
            "|org|org_dummy    |\n",
            "+---+-------------+\n",
            "|ORD|(7,[0],[1.0])|\n",
            "|SFO|(7,[1],[1.0])|\n",
            "|JFK|(7,[2],[1.0])|\n",
            "|LGA|(7,[3],[1.0])|\n",
            "|SJC|(7,[4],[1.0])|\n",
            "|SMF|(7,[5],[1.0])|\n",
            "|TUS|(7,[6],[1.0])|\n",
            "|OGG|(7,[],[])    |\n",
            "+---+-------------+\n",
            "\n",
            "807.6236144857218\n",
            "intercept= 15.874970289051719 which is the average minutes on ground at OGG airport.\n",
            "68.30203831360004\n",
            "62.631958009116566 \n",
            "\n",
            "coefficients =  [0.07429203272889287,28.459446518945672,20.44820481368227,52.42706802454832,46.75698772006485,18.27609704760683,15.532666664169884,17.73219653047847] has  8 slopes\n",
            "p-values = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1M1KrHyRHFH",
        "outputId": "01ad21f4-802f-436d-a756-f0fb3d0bb7b2"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\r\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\r\n",
        "\r\n",
        "df = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "df = df.withColumn(\"km\", df[\"mile\"]*1.609344).drop(\"mile\")\r\n",
        "df = df.dropna()\r\n",
        "df = StringIndexer(inputCol=\"carrier\",  outputCol=\"carrier_index\").fit(df).transform(df)\r\n",
        "df = StringIndexer(inputCol=\"org\", outputCol=\"org_index\").fit(df).transform(df)\r\n",
        "onehots = OneHotEncoder(inputCols=[\"carrier_index\", \"org_index\"], outputCols=[\"carrier_dummy\", \"org_dummy\"])\r\n",
        "df = onehots.fit(df).transform(df)\r\n",
        "\r\n",
        "# Including many as features.\r\n",
        "### ห้ามเผลอใส่ตัวแปรต้น เป็นตัวเดียวกับ label เพราะจะกลายเป็น spurious perfect fit\r\n",
        "\r\n",
        "df = VectorAssembler(inputCols=[\"mon\", \"dom\", \"dow\", \"flight\", \"depart\",'delay','km','carrier_dummy', 'org_dummy'], outputCol='features').transform(df)\r\n",
        "df[[\"delay\",\"km\", \"carrier_dummy\", \"org_dummy\", \"features\"]].show(5, truncate=False)\r\n",
        "df_train, df_test = df.randomSplit([0.8, 0.2], seed=42)\r\n",
        "\r\n",
        "# Create a regression object and train on training data\r\n",
        "regression = LinearRegression(labelCol=\"duration\").fit(df_train)\r\n",
        "\r\n",
        "# Create predictions for the testing data\r\n",
        "predictions = regression.transform(df_test)\r\n",
        "\r\n",
        "# Calculate the RMSE on testing data\r\n",
        "print(\"RMSE = \", RegressionEvaluator(labelCol=\"duration\").evaluate(predictions))\r\n",
        "predictions[[\"duration\", \"prediction\"]].show(5, truncate=False)\r\n",
        "\r\n",
        "# p-values (including intercepts) จะเห็นว่า ใส่มั่วๆ หลายตัวไม่ sig\r\n",
        "print(\"p-values =\", regression.summary.pValues)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+------------------+-------------+-------------+-----------------------------------------------------------------------------------+\n",
            "|delay|km                |carrier_dummy|org_dummy    |features                                                                           |\n",
            "+-----+------------------+-------------+-------------+-----------------------------------------------------------------------------------+\n",
            "|30   |508.55270400000006|(8,[0],[1.0])|(7,[0],[1.0])|(22,[1,2,3,4,5,6,7,15],[22.0,2.0,1107.0,16.33,30.0,508.55270400000006,1.0,1.0])    |\n",
            "|-8   |542.348928        |(8,[0],[1.0])|(7,[1],[1.0])|(22,[0,1,2,3,4,5,6,7,16],[2.0,20.0,4.0,226.0,6.17,-8.0,542.348928,1.0,1.0])        |\n",
            "|-5   |1989.149184       |(8,[1],[1.0])|(7,[0],[1.0])|(22,[0,1,2,3,4,5,6,8,15],[9.0,13.0,1.0,419.0,10.33,-5.0,1989.149184,1.0,1.0])      |\n",
            "|2    |885.1392000000001 |(8,[0],[1.0])|(7,[1],[1.0])|(22,[0,1,2,3,4,5,6,7,16],[5.0,2.0,1.0,704.0,7.98,2.0,885.1392000000001,1.0,1.0])   |\n",
            "|54   |1179.6491520000002|(8,[1],[1.0])|(7,[0],[1.0])|(22,[0,1,2,3,4,5,6,8,15],[7.0,2.0,6.0,380.0,10.83,54.0,1179.6491520000002,1.0,1.0])|\n",
            "+-----+------------------+-------------+-------------+-----------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "RMSE =  10.70494341630239\n",
            "+--------+------------------+\n",
            "|duration|prediction        |\n",
            "+--------+------------------+\n",
            "|560     |552.3867545457149 |\n",
            "|310     |306.33709787756897|\n",
            "|165     |149.46915933116668|\n",
            "|120     |129.40568886876568|\n",
            "|240     |227.24301105091072|\n",
            "+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "p-values = [6.021252385579601e-10, 0.08447073101257851, 0.17776504355563016, 4.483673432531532e-10, 0.0, 0.0009217605502500614, 0.0, 2.220446049250313e-16, 0.0, 1.85856410706009e-06, 0.0, 0.0, 6.503186172235331e-06, 0.0, 0.026677441134473012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdUvDENRoEet",
        "outputId": "24e1d0cd-f0d8-412a-b953-acae7c814773"
      },
      "source": [
        "# Bucketizing\r\n",
        "from pyspark.ml.feature import Bucketizer, OneHotEncoder, VectorAssembler, StringIndexer\r\n",
        "from pyspark.ml.regression import LinearRegression\r\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
        "\r\n",
        "flights = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "flights = flights.withColumn(\"km\", flights[\"mile\"]*1.609344).drop(\"mile\")\r\n",
        "flights = StringIndexer(inputCol=\"org\", outputCol=\"org_index\").fit(flights).transform(flights)\r\n",
        "\r\n",
        "# Create buckets at 3 hour intervals through the day\r\n",
        "buckets = Bucketizer(splits=[0, 3, 6, 9, 12, 15, 18, 21, 24], inputCol=\"depart\", outputCol=\"depart_bucket\")\r\n",
        "\r\n",
        "# Bucket the departure times\r\n",
        "bucketed = buckets.transform(flights)\r\n",
        "bucketed.select(\"depart\", \"depart_bucket\").show(5)\r\n",
        "\r\n",
        "# Create a one-hot encoder\r\n",
        "onehot = OneHotEncoder(inputCol=\"depart_bucket\", outputCol=\"depart_dummy\")\r\n",
        "\r\n",
        "# One-hot encode the bucketed departure times\r\n",
        "flights_onehot = onehot.fit(bucketed).transform(bucketed)\r\n",
        "flights_onehot.select(\"depart\", \"depart_bucket\", \"depart_dummy\").show(5)\r\n",
        "\r\n",
        "# Include org_dummy\r\n",
        "flights = OneHotEncoder(inputCol=\"org_index\", outputCol=\"org_dummy\").fit(flights_onehot).transform(flights_onehot)\r\n",
        "flights = VectorAssembler(inputCols=[\"km\", \"org_dummy\", \"depart_dummy\"], outputCol='features').transform(flights)\r\n",
        "flights[[\"features\"]].show(5, truncate=False)\r\n",
        "\r\n",
        "# Run regression\r\n",
        "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=42)\r\n",
        "\r\n",
        "# Create a regression object and train on training data\r\n",
        "regression = LinearRegression(labelCol=\"duration\").fit(flights_train)\r\n",
        "\r\n",
        "# Create predictions for the testing data\r\n",
        "predictions = regression.transform(flights_test)\r\n",
        "\r\n",
        "# Calculate the RMSE on testing data\r\n",
        "print(\"RMSE = \", RegressionEvaluator(labelCol=\"duration\").evaluate(predictions))\r\n",
        "predictions[[\"duration\", \"prediction\"]].show(5, truncate=False)\r\n",
        "\r\n",
        "# Average minutes on ground at OGG for flights departing between 21:00 and 24:00\r\n",
        "avg_eve_ogg = regression.intercept\r\n",
        "print(\"Average minutes on ground at OGG for flights departing between 21:00 and 24:00 = \", avg_eve_ogg)\r\n",
        "\r\n",
        "# Average minutes on ground at OGG for flights departing between 00:00 and 03:00\r\n",
        "avg_night_ogg = regression.intercept + regression.coefficients[8]\r\n",
        "print(\"Average minutes on ground at OGG for flights departing between 00:00 and 03:00 = \", avg_night_ogg)\r\n",
        "\r\n",
        "# Average minutes on ground at JFK for flights departing between 00:00 and 03:00\r\n",
        "avg_night_jfk = regression.intercept + regression.coefficients[3] + regression.coefficients[8]\r\n",
        "print(\"Average minutes on ground at JFK for flights departing between 00:00 and 03:00 = \", avg_night_jfk)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-------------+\n",
            "|depart|depart_bucket|\n",
            "+------+-------------+\n",
            "|  9.48|          3.0|\n",
            "| 16.33|          5.0|\n",
            "|  6.17|          2.0|\n",
            "| 10.33|          3.0|\n",
            "|  8.92|          2.0|\n",
            "+------+-------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------+-------------+-------------+\n",
            "|depart|depart_bucket| depart_dummy|\n",
            "+------+-------------+-------------+\n",
            "|  9.48|          3.0|(7,[3],[1.0])|\n",
            "| 16.33|          5.0|(7,[5],[1.0])|\n",
            "|  6.17|          2.0|(7,[2],[1.0])|\n",
            "| 10.33|          3.0|(7,[3],[1.0])|\n",
            "|  8.92|          2.0|(7,[2],[1.0])|\n",
            "+------+-------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------------------------------------------+\n",
            "|features                                  |\n",
            "+------------------------------------------+\n",
            "|(15,[0,3,11],[3464.917632,1.0,1.0])       |\n",
            "|(15,[0,1,13],[508.55270400000006,1.0,1.0])|\n",
            "|(15,[0,2,10],[542.348928,1.0,1.0])        |\n",
            "|(15,[0,1,11],[1989.149184,1.0,1.0])       |\n",
            "|(15,[0,1,10],[415.210752,1.0,1.0])        |\n",
            "+------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "RMSE =  10.741723052046062\n",
            "+--------+------------------+\n",
            "|duration|prediction        |\n",
            "+--------+------------------+\n",
            "|560     |552.4171964917089 |\n",
            "|310     |316.83085280545896|\n",
            "|90      |82.27835459121793 |\n",
            "|130     |134.21545494806196|\n",
            "|251     |242.29683426343937|\n",
            "+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Average minutes on ground at OGG for flights departing between 21:00 and 24:00 =  10.396859027799826\n",
            "Average minutes on ground at OGG for flights departing between 00:00 and 03:00 =  -4.295710321504497\n",
            "Average minutes on ground at JFK for flights departing between 00:00 and 03:00 =  47.353906540272014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aC8U59eLG7S",
        "outputId": "90104359-e296-48c6-e982-55421850ff0e"
      },
      "source": [
        "### Penalty for adding too many independent variables\r\n",
        "from pyspark.ml.regression import LinearRegression\r\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, Bucketizer\r\n",
        "\r\n",
        "flights = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "flights = flights.withColumn(\"km\", flights[\"mile\"]*1.609344).drop(\"mile\")\r\n",
        "flights = flights.dropna() ถ้าเราไม่เอาคอลัมน์ที่มี missing values มาใส่ใน vector assembler ก็ไม่จำเป็นต้อง dropna\r\n",
        "\r\n",
        "# StringIndexer เขียนรวมได้ แต่ต้องใช้ inputCols=['colname1', 'colname2'] outputCols=['colname', 'colname2']\r\n",
        "flights = StringIndexer(inputCols=[\"carrier\", \"org\"],  outputCols=[\"carrier_idx\", \"org_idx\"]).fit(flights).transform(flights)\r\n",
        "\r\n",
        "# Bucketizer ต้องมีตัวปิดตัวสุดท้าย ถ้า splits ค่าต่างกัน ต้องเขียนแยก\r\n",
        "flights = Bucketizer(splits=[0, 3, 6, 9, 12, 15, 18, 21, 24], inputCol=\"depart\", outputCol=\"depart_bucket\").transform(flights)\r\n",
        "flights = Bucketizer(splits=[0, 1, 2, 3, 4, 5, 6, 7], inputCol=\"dow\", outputCol=\"dow_bucket\").transform(flights)\r\n",
        "flights = Bucketizer(splits=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], inputCol=\"mon\", outputCol=\"mon_bucket\").transform(flights)\r\n",
        "flights = OneHotEncoder(inputCols=[\"carrier_idx\", \"org_idx\", \"depart_bucket\", \"dow_bucket\", \"mon_bucket\"], outputCols=[\"carrier_dummy\", \"org_dummy\", \"depart_dummy\", \"dow_dummy\", \"mon_dummy\"]).fit(flights).transform(flights)\r\n",
        "\r\n",
        "# Including many as features.\r\n",
        "### ห้ามเผลอใส่ตัวแปรต้น เป็นตัวเดียวกับ label เพราะจะกลายเป็น spurious perfect fit\r\n",
        "\r\n",
        "## The instructor reduced the number of observations from 50,000 to 1,000 on DC's browser \r\n",
        "## without telling and this process change the frequency of some dummy variables.  \r\n",
        "## For example, OGG airport is a benchmark in the fulll dataset but TUS becomes a benchmark in the exercise.\r\n",
        "# https://campus.datacamp.com/courses/machine-learning-with-pyspark/regression-ebb2870c-a2cd-40a0-a282-5604fdb4bd1c?ex=13\r\n",
        "\r\n",
        "flights = VectorAssembler(inputCols=['km','org_dummy', 'depart_dummy', 'dow_dummy', 'mon_dummy'], outputCol='features').transform(flights)\r\n",
        "flights[[\"features\", \"duration\"]].show(5, truncate=False)\r\n",
        "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=42)\r\n",
        "\r\n",
        "\r\n",
        "# Fit linear regression model to training data\r\n",
        "regression = LinearRegression(labelCol='duration').fit(flights_train)\r\n",
        "\r\n",
        "# Make predictions on testing data\r\n",
        "predictions = regression.transform(flights_test)\r\n",
        "\r\n",
        "# Calculate the RMSE on testing data\r\n",
        "rmse = RegressionEvaluator(labelCol=\"duration\").evaluate(predictions)\r\n",
        "print(\"The test RMSE is\", rmse)\r\n",
        "\r\n",
        "# Look at the model coefficients\r\n",
        "coeffs = regression.coefficients\r\n",
        "print(coeffs)\r\n",
        "\r\n",
        "# Ridge regression จริงๆ elasticNetParam ค่าเริ่มต้นเป็น 0 ให้อยู่แล้ว Ridge คือแก้ที่ regParam\r\n",
        "ridge = LinearRegression(labelCol=\"duration\", elasticNetParam=0, regParam=0.1).fit(flights_train)\r\n",
        "\r\n",
        "# Lasso regression\r\n",
        "lasso = LinearRegression(labelCol=\"duration\", elasticNetParam=1, regParam=0.1).fit(flights_train)\r\n",
        "\r\n",
        "## exercise\r\n",
        "# Fit Lasso model (α = 1) to training data\r\n",
        "regression = LinearRegression(labelCol=\"duration\", regParam=1, elasticNetParam=1).fit(flights_train)\r\n",
        "\r\n",
        "# Calculate the RMSE on testing data\r\n",
        "rmse = RegressionEvaluator(labelCol=\"duration\").evaluate(regression.transform(flights_test))\r\n",
        "print(\"The test RMSE is\", rmse)\r\n",
        "\r\n",
        "# Look at the model coefficients\r\n",
        "coeffs = regression.coefficients\r\n",
        "print(coeffs)\r\n",
        "\r\n",
        "# Number of zero coefficients\r\n",
        "zero_coeff = sum([beta==0 for beta in regression.coefficients])\r\n",
        "print(\"Number of coefficients equal to 0:\", zero_coeff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------+--------+\n",
            "|features                                                |duration|\n",
            "+--------------------------------------------------------+--------+\n",
            "|(32,[0,3,11],[3464.917632,1.0,1.0])                     |351     |\n",
            "|(32,[0,1,13,17,21],[508.55270400000006,1.0,1.0,1.0,1.0])|82      |\n",
            "|(32,[0,2,10,19,23],[542.348928,1.0,1.0,1.0,1.0])        |82      |\n",
            "|(32,[0,1,11,16,30],[1989.149184,1.0,1.0,1.0,1.0])       |195     |\n",
            "|(32,[0,1,10,20,25],[415.210752,1.0,1.0,1.0,1.0])        |65      |\n",
            "+--------------------------------------------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "The test RMSE is 10.640898098422634\n",
            "[0.07440105564835443,27.250459256776892,20.108551239680313,51.717706131537625,45.848484689332444,17.56510475699674,14.966511267973397,17.220627618309504,-15.252081036666475,0.4119592759848925,4.096780309434183,6.8582098158780775,4.619271845164633,8.7646491112333,8.72216284334477,0.13962677223553757,-0.13912740487790085,-0.40425303060786744,0.07633536397494753,0.00034480848488271297,-0.040044272425166186,-2.3408141609649857,-2.4348607862270946,-2.244912274343272,-3.674496196800548,-4.203905996297673,-4.358870000512794,-4.473743129130805,-4.413696698966029,-4.002205375826335,-2.816782447893692,-0.8491442514185962]\n",
            "The test RMSE is 11.635175214931525\n",
            "[0.07351229205781092,5.503246438394388,0.0,28.86970116596628,22.02044884732584,0.0,-2.398636717445728,0.0,0.0,0.0,0.0,0.0,0.0,1.0295454931694727,1.1414350036313592,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
            "Number of coefficients equal to 0: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpAcfB87qCez",
        "outputId": "6b164fd1-8e32-42ef-98da-000adf74f15e"
      },
      "source": [
        "### Pipeline: combining steps above\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.ml.regression import LinearRegression\r\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, Bucketizer\r\n",
        "\r\n",
        "flights = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "flights = flights.withColumn(\"km\", flights[\"mile\"]*1.609344).drop(\"mile\")\r\n",
        "\r\n",
        "# เริ่มเปลี่ยนตรงนี้\r\n",
        "indexer = StringIndexer(inputCols=[\"carrier\", \"org\"],  outputCols=[\"carrier_idx\", \"org_idx\"])#.fit(flights).transform(flights)\r\n",
        "onehot = OneHotEncoder(inputCols=[\"carrier_idx\", \"org_idx\", \"dow\"], outputCols=[\"carrier_dummy\", \"org_dummy\", \"dow_dummy\"])#.fit(flights).transform(flights)\r\n",
        "\r\n",
        "# ใส่มาแค่ \"km\", \"org_dummy\", \"dow_dummy\" ให้เหมือนในแบบฝึกหัด\r\n",
        "assembler = VectorAssembler(inputCols=['km','org_dummy', 'dow_dummy'], outputCol='features')#.transform(flights)\r\n",
        "regression = LinearRegression(labelCol=\"duration\")\r\n",
        "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\r\n",
        "\r\n",
        "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=42)\r\n",
        "# Train the pipeline on the training data\r\n",
        "pipeline = pipeline.fit(flights_train)\r\n",
        "\r\n",
        "# Make predictions on the testing data\r\n",
        "predictions = pipeline.transform(flights_test)\r\n",
        "\r\n",
        "# Access the regression coefficients\r\n",
        "pipeline.stages[3].coefficients"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseVector([0.0743, 28.4722, 20.4571, 52.4312, 46.7778, 18.2882, 15.5471, 17.7419, 0.2034, -0.083, -0.4135, 0.0509, -0.0633, -0.0856])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWj5oHKh8d2k",
        "outputId": "509b5fcd-90fb-4404-a48c-5bc807937cb0"
      },
      "source": [
        "# SMS Exercise again for reference convenience\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.sql.functions import regexp_replace\r\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\r\n",
        "from pyspark.ml.classification import LogisticRegression\r\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\r\n",
        "\r\n",
        "REGEX = '[_():;,.!?\\\\- 0-9]'\r\n",
        "\r\n",
        "sms = spark.read.csv(\"sms.csv\", inferSchema=True, sep=\";\")\r\n",
        "for i, j in zip([\"_c0\", \"_c1\", \"_c2\"], [\"id\", \"text\", \"label\"]):\r\n",
        "    sms = sms.withColumnRenamed(i, j)\r\n",
        "sms.show(5)\r\n",
        "\r\n",
        "# Remove punctuation (REGEX provided) and numbers\r\n",
        "sms = sms.withColumn('text', regexp_replace(\"text\", '[_():;,.!?\\\\- 0-9]', ' '))\r\n",
        "\r\n",
        "# Merge multiple spaces\r\n",
        "sms = sms.withColumn('text', regexp_replace(\"text\", ' +', ' '))\r\n",
        "\r\n",
        "# Split the text into words\r\n",
        "sms = Tokenizer(inputCol='text', outputCol=\"words\").transform(sms)\r\n",
        "sms.show(4, truncate=False)\r\n",
        "\r\n",
        "# Remove stop words.\r\n",
        "sms = StopWordsRemover(inputCol=\"words\", outputCol=\"terms\").transform(sms)\r\n",
        "\r\n",
        "# Apply the hashing trick\r\n",
        "sms = HashingTF(inputCol=\"terms\", outputCol=\"hash\", numFeatures=1024).transform(sms)\r\n",
        "\r\n",
        "# Convert hashed symbols to TF-IDF\r\n",
        "sms = IDF(inputCol=\"hash\", outputCol=\"features\").fit(sms).transform(sms)\r\n",
        "      \r\n",
        "sms.select('terms', 'features').show(4, truncate=False)\r\n",
        "\r\n",
        "# Split the data into training and testing sets\r\n",
        "sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed=13)\r\n",
        "\r\n",
        "# Fit a Logistic Regression model to the training data\r\n",
        "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\r\n",
        "\r\n",
        "# Make predictions on the testing data\r\n",
        "prediction = logistic.transform(sms_test)\r\n",
        "\r\n",
        "# Create a confusion matrix, comparing predictions to known labels\r\n",
        "prediction.groupBy(\"label\", \"prediction\").count().show()\r\n",
        "\r\n",
        "TP = prediction.where(\"label = 1 AND prediction = 1\").count()\r\n",
        "TN = prediction.where(\"label = 0 AND prediction = 0\").count()\r\n",
        "FP = prediction.where(\"label = 0 AND prediction = 1\").count()\r\n",
        "FN = prediction.where(\"label = 1 AND prediction = 0\").count()\r\n",
        "print(f\"The accuracy is {(TP+TN)/(TP+TN+FP+FN)*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+-----+\n",
            "| id|                text|label|\n",
            "+---+--------------------+-----+\n",
            "|  1|Sorry, I'll call ...|    0|\n",
            "|  2|Dont worry. I gue...|    0|\n",
            "|  3|Call FREEPHONE 08...|    1|\n",
            "|  4|Win a 1000 cash p...|    1|\n",
            "|  5|Go until jurong p...|    0|\n",
            "+---+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---+----------------------------------+-----+------------------------------------------+\n",
            "|id |text                              |label|words                                     |\n",
            "+---+----------------------------------+-----+------------------------------------------+\n",
            "|1  |Sorry I'll call later in meeting  |0    |[sorry, i'll, call, later, in, meeting]   |\n",
            "|2  |Dont worry I guess he's busy      |0    |[dont, worry, i, guess, he's, busy]       |\n",
            "|3  |Call FREEPHONE now                |1    |[call, freephone, now]                    |\n",
            "|4  |Win a cash prize or a prize worth |1    |[win, a, cash, prize, or, a, prize, worth]|\n",
            "+---+----------------------------------+-----+------------------------------------------+\n",
            "only showing top 4 rows\n",
            "\n",
            "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|terms                           |features                                                                                            |\n",
            "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|[sorry, call, later, meeting]   |(1024,[138,384,577,996],[2.273418200008753,3.6288353225642043,3.5890949939146903,4.104259019279279])|\n",
            "|[dont, worry, guess, busy]      |(1024,[215,233,276,329],[3.9913186080986836,3.3790235241678332,4.734227298217693,4.58299632849377]) |\n",
            "|[call, freephone]               |(1024,[133,138],[5.367951058306837,2.273418200008753])                                              |\n",
            "|[win, cash, prize, prize, worth]|(1024,[31,47,62,389],[3.6632029660684124,4.754846585420428,4.072170704727778,7.064594791043114])    |\n",
            "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 4 rows\n",
            "\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0|   41|\n",
            "|    0|       0.0|  948|\n",
            "|    1|       1.0|  105|\n",
            "|    0|       1.0|    2|\n",
            "+-----+----------+-----+\n",
            "\n",
            "The accuracy is 96.08%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0makV9IW9qDJ",
        "outputId": "f6019f8c-dc5f-4ad9-88bd-ec1a5205f038"
      },
      "source": [
        "### SMS spam pipeline\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.sql.functions import regexp_replace\r\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\r\n",
        "from pyspark.ml.classification import LogisticRegression\r\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\r\n",
        "\r\n",
        "# Prepare data as in the exercise\r\n",
        "REGEX = '[_():;,.!?\\\\- 0-9]'\r\n",
        "sms = spark.read.csv(\"sms.csv\", inferSchema=True, sep=\";\")\r\n",
        "for i, j in zip([\"_c0\", \"_c1\", \"_c2\"], [\"id\", \"text\", \"label\"]):\r\n",
        "    sms = sms.withColumnRenamed(i, j)\r\n",
        "# Remove punctuation (REGEX provided) and numbers\r\n",
        "sms = sms.withColumn('text', regexp_replace(\"text\", '[_():;,.!?\\\\- 0-9]', ' '))\r\n",
        "# Merge multiple spaces\r\n",
        "sms = sms.withColumn('text', regexp_replace(\"text\", ' +', ' '))\r\n",
        "sms.show(4, truncate=False)\r\n",
        "\r\n",
        "# Break text into tokens at non-word characters\r\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='words')\r\n",
        "\r\n",
        "# Remove stop words\r\n",
        "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='terms')\r\n",
        "\r\n",
        "# Apply the hashing trick and transform to TF-IDF\r\n",
        "hasher = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"hash\")\r\n",
        "idf = IDF(inputCol=hasher.getOutputCol(), outputCol=\"features\")\r\n",
        "\r\n",
        "# Split the data into training and testing sets\r\n",
        "sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed=13)\r\n",
        "\r\n",
        "# Create a logistic regression object and add everything to a pipeline\r\n",
        "logistic = LogisticRegression(regParam=0.2)\r\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])\r\n",
        "\r\n",
        "# Training data\r\n",
        "pipeline = pipeline.fit(sms_train)\r\n",
        "\r\n",
        "# Testing data\r\n",
        "predictions = pipeline.transform(sms_test)\r\n",
        "\r\n",
        "# Create a confusion matrix, comparing predictions to known labels\r\n",
        "predictions.groupBy(\"label\", \"prediction\").count().show()\r\n",
        "\r\n",
        "# คอลัมน์ label เหมือนจะใช้แค่กับ classification เท่านั้น "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------------------------------+-----+\n",
            "|id |text                              |label|\n",
            "+---+----------------------------------+-----+\n",
            "|1  |Sorry I'll call later in meeting  |0    |\n",
            "|2  |Dont worry I guess he's busy      |0    |\n",
            "|3  |Call FREEPHONE now                |1    |\n",
            "|4  |Win a cash prize or a prize worth |1    |\n",
            "+---+----------------------------------+-----+\n",
            "only showing top 4 rows\n",
            "\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0|   42|\n",
            "|    0|       0.0|  950|\n",
            "|    1|       1.0|  104|\n",
            "+-----+----------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9dw7TZV9ql1",
        "outputId": "317b8abb-243b-422b-d88c-e0d54d995048"
      },
      "source": [
        "### Cross Validation\r\n",
        "\r\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.ml.regression import LinearRegression\r\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, Bucketizer\r\n",
        "\r\n",
        "flights = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "flights = flights.withColumn(\"km\", flights[\"mile\"]*1.609344).drop(\"mile\")\r\n",
        "indexer = StringIndexer(inputCols=[\"carrier\", \"org\"],  outputCols=[\"carrier_idx\", \"org_idx\"])#.fit(flights).transform(flights)\r\n",
        "onehot = OneHotEncoder(inputCols=[\"carrier_idx\", \"org_idx\", \"dow\"], outputCols=[\"carrier_dummy\", \"org_dummy\", \"dow_dummy\"])#.fit(flights).transform(flights)\r\n",
        "assembler = VectorAssembler(inputCols=['km', \"org_dummy\"], outputCol='features')#.transform(flights)\r\n",
        "regression = LinearRegression(labelCol=\"duration\")\r\n",
        "evaluator = RegressionEvaluator(labelCol=\"duration\")\r\n",
        "piped_data = Pipeline(stages=[indexer, onehot, assembler]).fit(flights).transform(flights)\r\n",
        "flights_train, flights_test = piped_data.randomSplit([0.8, 0.2], seed=42)\r\n",
        "\r\n",
        "## ขั้นตอนนี้งงๆ ต้องทำ pipeline ก่อน split หรือ split ก่อน pipeline\r\n",
        "## รู้แต่ว่า ใน cross validation ต้องมี features มาก่อน เหมือนกับต้องเอา piped data ไป split ไม่ใช่เอา raw data มา split\r\n",
        "\r\n",
        "# Create a grid for parameter\r\n",
        "params = ParamGridBuilder().build()\r\n",
        "\r\n",
        "# Create the cross validator object: numFolds คือการเอา train data มาแบ่งเพื่อทำ cross validation\r\n",
        "# cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator, numFolds=10, seed=13)\r\n",
        "# estimator = regression ได้ ถ้าหากไม่ได้ทำ pipeline มา\r\n",
        "\r\n",
        "# ???\r\n",
        "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\r\n",
        "cv2 = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=10, seed=13)\r\n",
        "\r\n",
        "# Apply cross-validation to the training data\r\n",
        "cv = cv.fit(flights_train)\r\n",
        "cv.avgMetrics\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11.251745641698584]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSF9KF0uL4hk",
        "outputId": "c853be3e-33f1-45e1-f5a6-f34f7f3891ed"
      },
      "source": [
        "### Grid Search\r\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.ml.regression import LinearRegression\r\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, Bucketizer\r\n",
        "\r\n",
        "flights = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "flights = flights.withColumn(\"km\", flights[\"mile\"]*1.609344).drop(\"mile\")\r\n",
        "indexer = StringIndexer(inputCols=[\"carrier\", \"org\"],  outputCols=[\"carrier_idx\", \"org_idx\"])#.fit(flights).transform(flights)\r\n",
        "onehot = OneHotEncoder(inputCols=[\"carrier_idx\", \"org_idx\", \"dow\"], outputCols=[\"carrier_dummy\", \"org_dummy\", \"dow_dummy\"])#.fit(flights).transform(flights)\r\n",
        "assembler = VectorAssembler(inputCols=['km', \"org_dummy\"], outputCol='features')#.transform(flights)\r\n",
        "evaluator = RegressionEvaluator(labelCol=\"duration\")\r\n",
        "piped_data = Pipeline(stages=[indexer, onehot, assembler]).fit(flights).transform(flights)\r\n",
        "flights_train, flights_test = piped_data.randomSplit([0.8, 0.2], seed=42)\r\n",
        "\r\n",
        "regression = LinearRegression(labelCol=\"duration\")\r\n",
        "#predictions = regression.transform(flights_test)\r\n",
        "#params = ParamGridBuilder().addGrid(regression.fitIntercept, [True, False]).build()\r\n",
        "params = ParamGridBuilder().addGrid(regression.fitIntercept, [True, False]).addGrid(regression.regParam, [0.001, 0.01, 0.1, 1, 10]).addGrid(regression.elasticNetParam, [0, 0.25, 0.5, 0.75, 1]).build()\r\n",
        "cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator, numFolds=10, seed=13)\r\n",
        "# Apply cross-validation to the training data\r\n",
        "cv = cv.fit(flights_train)\r\n",
        "print(f'RMSE with intercept is {cv.avgMetrics[0]:.2f}.\\nRMSE without intercept is {cv.avgMetrics[1]:.2f}.')\r\n",
        "\r\n",
        "# Make prediction after cross validation\r\n",
        "predictions = cv.transform(flights_test)\r\n",
        "print(\"RMSE of tested data = \", RegressionEvaluator(labelCol=\"duration\").evaluate(predictions))\r\n",
        "\r\n",
        "# Retrieve the best parameter\r\n",
        "print(cv.bestModel.explainParam('fitIntercept'))\r\n",
        "print(cv.bestModel.explainParam(\"regParam\"))\r\n",
        "print(cv.bestModel.explainParam(\"elasticNetParam\"))\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE with intercept is 11.25.\n",
            "RMSE without intercept is 11.25.\n",
            "RMSE of tested data =  11.068149616453239\n",
            "fitIntercept: whether to fit an intercept term. (default: True, current: True)\n",
            "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.001)\n",
            "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.25)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGWb_DDF3oYw",
        "outputId": "fe439b92-fb3e-42db-8bc5-922abe466c76"
      },
      "source": [
        "### Exercise example on grid search\r\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.ml.regression import LinearRegression\r\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, Bucketizer\r\n",
        "\r\n",
        "flights = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "flights = flights.withColumn(\"km\", flights[\"mile\"]*1.609344).drop(\"mile\")\r\n",
        "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=42)\r\n",
        "indexer = StringIndexer(inputCols=[\"carrier\", \"org\"],  outputCols=[\"carrier_idx\", \"org_idx\"])#.fit(flights).transform(flights)\r\n",
        "onehot = OneHotEncoder(inputCols=[\"carrier_idx\", \"org_idx\", \"dow\"], outputCols=[\"carrier_dummy\", \"org_dummy\", \"dow_dummy\"])#.fit(flights).transform(flights)\r\n",
        "assembler = VectorAssembler(inputCols=['km', \"org_dummy\"], outputCol='features')#.transform(flights)\r\n",
        "regression = LinearRegression(labelCol=\"duration\")\r\n",
        "evaluator = RegressionEvaluator(labelCol=\"duration\")\r\n",
        "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\r\n",
        "\r\n",
        "# Create parameter grid\r\n",
        "params = ParamGridBuilder()\r\n",
        "\r\n",
        "# Add grids for two parameters\r\n",
        "params = params.addGrid(regression.regParam, [0.01, 0.1, 1.0, 10.0]) \\\r\n",
        "               .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0])\r\n",
        "\r\n",
        "# Build the parameter grid\r\n",
        "params = params.build()\r\n",
        "print('Number of models to be tested: ', len(params))\r\n",
        "\r\n",
        "# Create cross-validator\r\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\r\n",
        "cv = cv.fit(flights_train)\r\n",
        "\r\n",
        "# Get the best model from cross validation\r\n",
        "best_model = cv.bestModel\r\n",
        "\r\n",
        "# Look at the stages in the best model\r\n",
        "print(best_model.stages)\r\n",
        "\r\n",
        "# Get the parameters for the LinearRegression object in the best model\r\n",
        "best_model.stages[3].extractParamMap()\r\n",
        "\r\n",
        "# Generate predictions on testing data using the best model then calculate RMSE\r\n",
        "predictions = best_model.transform(flights_test)\r\n",
        "print(\"RMSE after cross validation = \", evaluator.evaluate(predictions))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of models to be tested:  12\n",
            "[StringIndexerModel: uid=StringIndexer_3dea0e38ba78, handleInvalid=error, numInputCols=2, numOutputCols=2, OneHotEncoderModel: uid=OneHotEncoder_800891249a32, dropLast=true, handleInvalid=error, numInputCols=3, numOutputCols=3, VectorAssembler_a4ae9bcd6f8b, LinearRegressionModel: uid=LinearRegression_3d286bc8fed4, numFeatures=8]\n",
            "RMSE after cross validation =  11.06828961932966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIRi9uqkVBpz",
        "outputId": "3bcbc0a5-3b75-430c-d549-f985bf2215e2"
      },
      "source": [
        "### SMS spam pipeline Grid Search\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.sql.functions import regexp_replace\r\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\r\n",
        "from pyspark.ml.classification import LogisticRegression\r\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\r\n",
        "\r\n",
        "# Prepare data as in the exercise\r\n",
        "REGEX = '[_():;,.!?\\\\- 0-9]'\r\n",
        "sms = spark.read.csv(\"sms.csv\", inferSchema=True, sep=\";\")\r\n",
        "for i, j in zip([\"_c0\", \"_c1\", \"_c2\"], [\"id\", \"text\", \"label\"]):\r\n",
        "    sms = sms.withColumnRenamed(i, j)\r\n",
        "# Remove punctuation (REGEX provided) and numbers\r\n",
        "sms = sms.withColumn('text', regexp_replace(\"text\", '[_():;,.!?\\\\- 0-9]', ' '))\r\n",
        "# Merge multiple spaces\r\n",
        "sms = sms.withColumn('text', regexp_replace(\"text\", ' +', ' '))\r\n",
        "sms.show(4, truncate=False)\r\n",
        "\r\n",
        "# Break text into tokens at non-word characters\r\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='words')\r\n",
        "\r\n",
        "# Remove stop words\r\n",
        "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='terms')\r\n",
        "\r\n",
        "# Apply the hashing trick and transform to TF-IDF\r\n",
        "hasher = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"hash\")\r\n",
        "idf = IDF(inputCol=hasher.getOutputCol(), outputCol=\"features\")\r\n",
        "\r\n",
        "# Create a logistic regression object and add everything to a pipeline\r\n",
        "logistic = LogisticRegression()\r\n",
        "binary_evaluator = BinaryClassificationEvaluator()\r\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])\r\n",
        "\r\n",
        "# Split the data into training and testing sets\r\n",
        "sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed=13)\r\n",
        "\r\n",
        "# Create parameter grid\r\n",
        "params = ParamGridBuilder()\r\n",
        "\r\n",
        "# Add grid for hashing trick parameters\r\n",
        "params = params.addGrid(hasher.numFeatures, [1024, 4096, 16384]) \\\r\n",
        "               .addGrid(hasher.binary, [True, False])\r\n",
        "\r\n",
        "# Add grid for logistic regression parameters\r\n",
        "params = params.addGrid(logistic.regParam, [0.01, 0.1, 1.0, 10.0]).addGrid(logistic.elasticNetParam, [0.0, 0.5, 1.0])\r\n",
        "\r\n",
        "# Build parameter grid\r\n",
        "params = params.build()\r\n",
        "\r\n",
        "# Training data\r\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=binary_evaluator, numFolds=5)\r\n",
        "cv = cv.fit(sms_train)\r\n",
        "\r\n",
        "# Get the best model from cross validation\r\n",
        "best_model = cv.bestModel\r\n",
        "\r\n",
        "# Look at the stages in the best model\r\n",
        "print(best_model.stages)\r\n",
        "\r\n",
        "# Get the parameters for the LinearRegression object in the best model\r\n",
        "best_model.stages[3].extractParamMap()\r\n",
        "\r\n",
        "# Generate predictions on testing data using the best model then calculate RMSE\r\n",
        "predictions = best_model.transform(sms_test)\r\n",
        "print(\"RMSE after cross validation = \", binary_evaluator.evaluate(predictions))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------------------------------+-----+\n",
            "|id |text                              |label|\n",
            "+---+----------------------------------+-----+\n",
            "|1  |Sorry I'll call later in meeting  |0    |\n",
            "|2  |Dont worry I guess he's busy      |0    |\n",
            "|3  |Call FREEPHONE now                |1    |\n",
            "|4  |Win a cash prize or a prize worth |1    |\n",
            "+---+----------------------------------+-----+\n",
            "only showing top 4 rows\n",
            "\n",
            "[Tokenizer_402a220b20f5, StopWordsRemover_862f39bc11e7, HashingTF_a0341b90f27b, IDFModel: uid=IDF_4da3519785f1, numDocs=4478, numFeatures=16384, LogisticRegressionModel: uid=LogisticRegression_8eb2cf6de984, numClasses=2, numFeatures=16384]\n",
            "RMSE after cross validation =  0.9933237202595511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp-9FM4NbBXJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29f83c1a-dbaa-4462-f0bf-1ceb71112a04"
      },
      "source": [
        "### Random Forest\r\n",
        "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, DecisionTreeClassifier\r\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, Bucketizer\r\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\r\n",
        "\r\n",
        "flights = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "flights = flights.withColumn(\"km\", flights[\"mile\"]*1.609344).drop(\"mile\")\r\n",
        "flights = flights.withColumn(\"label\", (flights[\"delay\"] >= 15).cast(\"integer\"))\r\n",
        "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=42)\r\n",
        "indexer = StringIndexer(inputCols=[\"carrier\", \"org\"],  outputCols=[\"carrier_idx\", \"org_idx\"])#.fit(flights).transform(flights)\r\n",
        "onehot = OneHotEncoder(inputCols=[\"carrier_idx\", \"org_idx\", \"dow\"], outputCols=[\"carrier_dummy\", \"org_dummy\", \"dow_dummy\"])#.fit(flights).transform(flights)\r\n",
        "assembler = VectorAssembler(inputCols=[\"duration\", 'km', \"org_dummy\"], outputCol='features')#.transform(flights)\r\n",
        "pipeline = Pipeline(stages=[indexer, onehot, assembler])\r\n",
        "piped_data = pipeline.fit(flights).transform(flights).dropna() # random forest ต้องมีคอลัมน์ label, features และต้องไม่มี missing values\r\n",
        "flights_train, flights_test = piped_data.randomSplit([0.8, 0.2], seed=42)\r\n",
        "\r\n",
        "# Create tree classifier\r\n",
        "forest = RandomForestClassifier(numTrees=5)\r\n",
        "gbt = GBTClassifier(maxIter=10)\r\n",
        "tree = DecisionTreeClassifier()\r\n",
        "\r\n",
        "# Fit the training data\r\n",
        "forest = forest.fit(flights_train)\r\n",
        "gbt = gbt.fit(flights_train)\r\n",
        "tree = tree.fit(flights_train)\r\n",
        "\r\n",
        "# How to access trees within forest?\r\n",
        "print(forest.trees)\r\n",
        "print(gbt.trees)\r\n",
        "\r\n",
        "# Consensus predictions\r\n",
        "predictions_forest = forest.transform(flights_test)\r\n",
        "predictions_gbt = gbt.transform(flights_test)\r\n",
        "predictions_tree = tree.transform(flights_test)\r\n",
        "predictions[[\"label\", \"probability\", \"prediction\"]].show(5, truncate=False)\r\n",
        "\r\n",
        "\r\n",
        "# Confusion matrix\r\n",
        "predictions_forest.groupBy(\"label\", \"prediction\").count().show()\r\n",
        "TP = predictions_forest.where(\"label = 1 AND prediction = 1\").count()\r\n",
        "TN = predictions_forest.where(\"label = 0 AND prediction = 0\").count()\r\n",
        "FP = predictions_forest.where(\"label = 0 AND prediction = 1\").count()\r\n",
        "FN = predictions_forest.where(\"label = 1 AND prediction = 0\").count()\r\n",
        "print(f\"The accuracy is of Random Forest is {(TP+TN)/(TP+TN+FP+FN)*100:.2f}%\")\r\n",
        "\r\n",
        "predictions_gbt.groupBy(\"label\", \"prediction\").count().show()\r\n",
        "TP = predictions_gbt.where(\"label = 1 AND prediction = 1\").count()\r\n",
        "TN = predictions_gbt.where(\"label = 0 AND prediction = 0\").count()\r\n",
        "FP = predictions_gbt.where(\"label = 0 AND prediction = 1\").count()\r\n",
        "FN = predictions_gbt.where(\"label = 1 AND prediction = 0\").count()\r\n",
        "print(f\"The accuracy is of Gradient Boosted Trees is {(TP+TN)/(TP+TN+FP+FN)*100:.2f}%\")\r\n",
        "\r\n",
        "predictions_tree.groupBy(\"label\", \"prediction\").count().show()\r\n",
        "TP = predictions_tree.where(\"label = 1 AND prediction = 1\").count()\r\n",
        "TN = predictions_tree.where(\"label = 0 AND prediction = 0\").count()\r\n",
        "FP = predictions_tree.where(\"label = 0 AND prediction = 1\").count()\r\n",
        "FN = predictions_tree.where(\"label = 1 AND prediction = 0\").count()\r\n",
        "print(f\"The accuracy is of Decision Trees is {(TP+TN)/(TP+TN+FP+FN)*100:.2f}%\")\r\n",
        "\r\n",
        "# Compare AUC on testing data\r\n",
        "evaluator = BinaryClassificationEvaluator()\r\n",
        "print(\"The AUC of Decision Tree = \", evaluator.evaluate(predictions_tree))\r\n",
        "print(\"The AUC of Gradient Boosted Tree = \", evaluator.evaluate(predictions_gbt))\r\n",
        "print(\"The AUC of Random Forest = \", evaluator.evaluate(predictions_forest))\r\n",
        "\r\n",
        "# Feature importances: index เรียงตามลำดับของตัวแปรที่ใส่ใน inputCols ขั้นตอน assembele\r\n",
        "display(forest.featureImportances)\r\n",
        "flights_train[[\"org\", \"org_dummy\"]].distinct().sort(\"org_idx\").show()\r\n",
        "print(f\"index เรียงตามลำดับของตัวแปรที่ใส่ใน inputCols ขั้นตอน assembele \\nแสดงว่า duration = {forest.featureImportances[0]:.2f} มีผลมากที่สุด ส่วนสนามบิน ORG มีผลอันดับสอง = {forest.featureImportances[2]:.2f} ส่วนสนามบิน LGA มีผลน้อยที่สุด = {forest.featureImportances[5]:.2f}.\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DecisionTreeClassificationModel: uid=dtc_0bade214d955, depth=5, numNodes=27, numClasses=2, numFeatures=9, DecisionTreeClassificationModel: uid=dtc_b686d4d2abe8, depth=5, numNodes=19, numClasses=2, numFeatures=9, DecisionTreeClassificationModel: uid=dtc_e471dbde3cea, depth=5, numNodes=11, numClasses=2, numFeatures=9, DecisionTreeClassificationModel: uid=dtc_d18e5513250c, depth=4, numNodes=17, numClasses=2, numFeatures=9, DecisionTreeClassificationModel: uid=dtc_1174bfa385ae, depth=5, numNodes=21, numClasses=2, numFeatures=9]\n",
            "[DecisionTreeRegressionModel: uid=dtr_15dc9ea878cf, depth=5, numNodes=63, numFeatures=9, DecisionTreeRegressionModel: uid=dtr_6395b7c83f2d, depth=5, numNodes=49, numFeatures=9, DecisionTreeRegressionModel: uid=dtr_45e460e87f21, depth=5, numNodes=49, numFeatures=9, DecisionTreeRegressionModel: uid=dtr_3070118b8781, depth=5, numNodes=57, numFeatures=9, DecisionTreeRegressionModel: uid=dtr_bd53fc515015, depth=5, numNodes=49, numFeatures=9, DecisionTreeRegressionModel: uid=dtr_974e01f3df82, depth=5, numNodes=51, numFeatures=9, DecisionTreeRegressionModel: uid=dtr_28c62d6f4ef2, depth=5, numNodes=47, numFeatures=9, DecisionTreeRegressionModel: uid=dtr_40f60a0cf3fc, depth=5, numNodes=49, numFeatures=9, DecisionTreeRegressionModel: uid=dtr_6c6a974eaa84, depth=5, numNodes=41, numFeatures=9, DecisionTreeRegressionModel: uid=dtr_0294ccd0934f, depth=5, numNodes=49, numFeatures=9]\n",
            "+-----+----------------------------------------+----------+\n",
            "|label|probability                             |prediction|\n",
            "+-----+----------------------------------------+----------+\n",
            "|1    |[0.4186667946465488,0.5813332053534512] |1.0       |\n",
            "|1    |[0.48923599541953716,0.5107640045804629]|1.0       |\n",
            "|0    |[0.4461282626338342,0.5538717373661657] |1.0       |\n",
            "|0    |[0.4465214265661099,0.5534785734338901] |1.0       |\n",
            "|1    |[0.4461282626338342,0.5538717373661657] |1.0       |\n",
            "+-----+----------------------------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 1009|\n",
            "|    0|       0.0| 1614|\n",
            "|    1|       1.0| 3851|\n",
            "|    0|       1.0| 2947|\n",
            "+-----+----------+-----+\n",
            "\n",
            "The accuracy is of Random Forest is 58.01%\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 1341|\n",
            "|    0|       0.0| 1914|\n",
            "|    1|       1.0| 3519|\n",
            "|    0|       1.0| 2647|\n",
            "+-----+----------+-----+\n",
            "\n",
            "The accuracy is of Gradient Boosted Trees is 57.67%\n",
            "+-----+----------+-----+\n",
            "|label|prediction|count|\n",
            "+-----+----------+-----+\n",
            "|    1|       0.0| 1454|\n",
            "|    0|       0.0| 2025|\n",
            "|    1|       1.0| 3406|\n",
            "|    0|       1.0| 2536|\n",
            "+-----+----------+-----+\n",
            "\n",
            "The accuracy is of Decision Trees is 57.65%\n",
            "The AUC of Decision Tree =  0.5367598840771147\n",
            "The AUC of Gradient Boosted Tree =  0.6054977655430774\n",
            "The AUC of Random Forest =  0.600070173586581\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SparseVector(9, {0: 0.2227, 1: 0.1373, 2: 0.1668, 3: 0.1, 4: 0.0317, 5: 0.0308, 6: 0.141, 7: 0.1058, 8: 0.0639})"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "+---+-------------+\n",
            "|org|    org_dummy|\n",
            "+---+-------------+\n",
            "|ORD|(7,[0],[1.0])|\n",
            "|SFO|(7,[1],[1.0])|\n",
            "|JFK|(7,[2],[1.0])|\n",
            "|LGA|(7,[3],[1.0])|\n",
            "|SJC|(7,[4],[1.0])|\n",
            "|SMF|(7,[5],[1.0])|\n",
            "|TUS|(7,[6],[1.0])|\n",
            "|OGG|    (7,[],[])|\n",
            "+---+-------------+\n",
            "\n",
            "index เรียงตามลำดับของตัวแปรที่ใส่ใน inputCols ขั้นตอน assembele \n",
            "แสดงว่า duration = 0.22 มีผลมากที่สุด ส่วนสนามบิน ORG มีผลอันดับสอง = 0.17 ส่วนสนามบิน LGA มีผลน้อยที่สุด = 0.03.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBTAQ63vXfoT",
        "outputId": "17e78971-a46c-4b75-bfca-b9587a618642"
      },
      "source": [
        "### Random Forest Cross validation เอาตามโจทย์เลย ไม่มีการทำ indexer, onehot ใดๆทั้งสิ้น\r\n",
        "from pyspark.ml.classification import RandomForestClassifier\r\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\r\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\r\n",
        "\r\n",
        "flights = spark.read.csv(\"flights.csv\", header=True, inferSchema=True, nullValue=\"NA\")\r\n",
        "flights = flights.withColumn(\"km\", flights[\"mile\"]*1.609344).drop(\"mile\")\r\n",
        "flights = flights.withColumn(\"label\", (flights[\"delay\"] >= 15).cast(\"integer\"))\r\n",
        "flights = flights[[\"mon\", \"depart\", \"duration\", \"label\"]].dropna()\r\n",
        "flights = VectorAssembler(inputCols=[\"mon\", \"depart\", \"duration\"], outputCol='features').transform(flights)\r\n",
        "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=42)\r\n",
        "forest = RandomForestClassifier()\r\n",
        "evaluator = BinaryClassificationEvaluator()\r\n",
        "\r\n",
        "# Create a parameter grid\r\n",
        "# featureSubsetStrategy — the number of features to consider for splitting at each node.\r\n",
        "# maxDepth — the maximum number of splits along any branch.\r\n",
        "params = ParamGridBuilder() \\\r\n",
        "            .addGrid(forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']) \\\r\n",
        "            .addGrid(forest.maxDepth, [2, 5, 10, 20, 25]) \\\r\n",
        "            .build()\r\n",
        "\r\n",
        "# Create a cross-validator\r\n",
        "cv = CrossValidator(estimator=forest, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\r\n",
        "cv = cv.fit(flights_train)\r\n",
        "\r\n",
        "# Get the best model from cross validation\r\n",
        "best_model = cv.bestModel\r\n",
        "\r\n",
        "# Generate predictions on testing data using the best model then calculate RMSE\r\n",
        "predictions = best_model.transform(flights_test)\r\n",
        "print(\"AUC after cross validation = \", evaluator.evaluate(predictions))\r\n",
        "print(\"Optimal parameters\")\r\n",
        "print(best_model.explainParam('featureSubsetStrategy'))\r\n",
        "print(best_model.explainParam(\"maxDepth\"))\r\n",
        "\r\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC after cross validation =  0.6796854991144257\n",
            "Optimal parameters\n",
            "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto, current: onethird)\n",
            "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5, current: 10)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}