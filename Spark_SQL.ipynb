{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_SQL",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d5ivXKechPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0272d3-2aa0-4629-e91d-5b02e958a743"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 64kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 49.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=f155ed292c5391de5329ac4a30b5f27fd4003fdec9f25007848efacba899a06f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSKBMk9Ec05o"
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark import SparkContext\r\n",
        "sc = SparkContext()\r\n",
        "spark = SparkSession.builder.master('local[*]').appName('first_spark_application').getOrCreate()\r\n",
        "\r\n",
        "# Thanks to Mike Metzger, a good instructor in DC\r\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z3xuOrGdQuR",
        "outputId": "1d4ebae1-c050-49b0-d38d-6776fc11bc26"
      },
      "source": [
        "df = spark.read.csv(\"trainsched.txt\", header=True, inferSchema=True, sep='\\t')\r\n",
        "df.createOrReplaceTempView(\"schedule\")\r\n",
        "spark.sql(\"SELECT * FROM schedule WHERE station= 'San Jose'\").show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------+-----+--------+\n",
            "|train_id| station| time|diff_min|\n",
            "+--------+--------+-----+--------+\n",
            "|     324|San Jose|9:05a|    null|\n",
            "|     217|San Jose|6:59a|    null|\n",
            "+--------+--------+-----+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq2sr8pCd7j7",
        "outputId": "aba9f57f-d04f-4f22-9f8e-09a13cca5e3a"
      },
      "source": [
        "# Inspect Schema\r\n",
        "result1 = spark.sql(\"SHOW COLUMNS FROM schedule\")\r\n",
        "result2 = spark.sql(\"SELECT * FROM schedule LIMIT 0\")\r\n",
        "result3 = spark.sql(\"DESCRIBE schedule\")\r\n",
        "for i in [result1, result2, result3]:\r\n",
        "    i.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|col_name|\n",
            "+--------+\n",
            "|train_id|\n",
            "| station|\n",
            "|    time|\n",
            "|diff_min|\n",
            "+--------+\n",
            "\n",
            "+--------+-------+----+--------+\n",
            "|train_id|station|time|diff_min|\n",
            "+--------+-------+----+--------+\n",
            "+--------+-------+----+--------+\n",
            "\n",
            "+--------+---------+-------+\n",
            "|col_name|data_type|comment|\n",
            "+--------+---------+-------+\n",
            "|train_id|      int|   null|\n",
            "| station|   string|   null|\n",
            "|    time|   string|   null|\n",
            "|diff_min|   string|   null|\n",
            "+--------+---------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWv_135QeE6a",
        "outputId": "359ec28a-88e9-4a97-e4ab-7e477dedf067"
      },
      "source": [
        "#OVER(ORDER BY)\r\n",
        "query = \"\"\"SELECT *, \r\n",
        "                  LEAD(time, 1) OVER(ORDER BY time) AS time_next\r\n",
        "           FROM schedule \r\n",
        "           WHERE train_id = 324\"\"\"\r\n",
        "spark.sql(query).show()\r\n",
        "\r\n",
        "query = \"\"\"SELECT *,\r\n",
        "                  LEAD(time, 1) OVER(PARTITION BY train_id ORDER BY time) AS time_next\r\n",
        "                  FROM schedule\r\n",
        "                  ORDER BY train_id, time\"\"\"\r\n",
        "spark.sql(query).show() \r\n",
        "                 "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------+-----+--------+---------+\n",
            "|train_id|      station| time|diff_min|time_next|\n",
            "+--------+-------------+-----+--------+---------+\n",
            "|     324|San Francisco|7:59a|       4|    8:03a|\n",
            "|     324|  22nd Street|8:03a|      13|    8:16a|\n",
            "|     324|     Millbrae|8:16a|       8|    8:24a|\n",
            "|     324|    Hillsdale|8:24a|       7|    8:31a|\n",
            "|     324| Redwood City|8:31a|       6|    8:37a|\n",
            "|     324|    Palo Alto|8:37a|      28|    9:05a|\n",
            "|     324|     San Jose|9:05a|    null|     null|\n",
            "+--------+-------------+-----+--------+---------+\n",
            "\n",
            "+--------+-------------+-----+--------+---------+\n",
            "|train_id|      station| time|diff_min|time_next|\n",
            "+--------+-------------+-----+--------+---------+\n",
            "|     217|       Gilroy|6:06a|       9|    6:15a|\n",
            "|     217|   San Martin|6:15a|       6|    6:21a|\n",
            "|     217|  Morgan Hill|6:21a|      15|    6:36a|\n",
            "|     217| Blossom Hill|6:36a|       6|    6:42a|\n",
            "|     217|      Capitol|6:42a|       8|    6:50a|\n",
            "|     217|       Tamien|6:50a|       9|    6:59a|\n",
            "|     217|     San Jose|6:59a|    null|     null|\n",
            "|     324|San Francisco|7:59a|       4|    8:03a|\n",
            "|     324|  22nd Street|8:03a|      13|    8:16a|\n",
            "|     324|     Millbrae|8:16a|       8|    8:24a|\n",
            "|     324|    Hillsdale|8:24a|       7|    8:31a|\n",
            "|     324| Redwood City|8:31a|       6|    8:37a|\n",
            "|     324|    Palo Alto|8:37a|      28|    9:05a|\n",
            "|     324|     San Jose|9:05a|    null|     null|\n",
            "+--------+-------------+-----+--------+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-hqAlpRk1-j",
        "outputId": "6d77972c-bf3f-47d8-8a81-52bfb320c85e"
      },
      "source": [
        "df = spark.read.csv(\"trainsched.txt\", header=True, inferSchema=True, sep='\\t')\r\n",
        "df.createOrReplaceTempView(\"schedule\")\r\n",
        "query = \"\"\"SELECT *,\r\n",
        "                  LEAD(time, 1) OVER(PARTITION BY train_id ORDER BY time) AS time_next\r\n",
        "                  FROM schedule\r\n",
        "                  ORDER BY train_id, time\"\"\"\r\n",
        "\r\n",
        "df = spark.sql(query)\r\n",
        "df = df[[\"train_id\", \"station\", \"time\", \"time_next\", \"diff_min\"]]\r\n",
        "df.show()\r\n",
        "\r\n",
        "# Running sums using window function SQL\r\n",
        "query = \"\"\"\r\n",
        "SELECT *, \r\n",
        "SUM(diff_min) OVER (PARTITION BY train_id ORDER BY time) AS running_total\r\n",
        "FROM schedule\r\n",
        "ORDER BY train_id, time\r\n",
        "\"\"\"\r\n",
        "spark.sql(query).show()\r\n",
        "\r\n",
        "# PARTITION BY is important\r\n",
        "query = \"\"\"\r\n",
        "SELECT \r\n",
        "ROW_NUMBER() OVER (PARTITION BY train_id ORDER BY time) AS row,\r\n",
        "train_id, \r\n",
        "station, \r\n",
        "time, \r\n",
        "LEAD(time,1) OVER (PARTITION BY train_id ORDER BY time) AS time_next \r\n",
        "FROM schedule\r\n",
        "ORDER BY train_id, time\r\n",
        "\"\"\"\r\n",
        "spark.sql(query).show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------+-----+---------+--------+\n",
            "|train_id|      station| time|time_next|diff_min|\n",
            "+--------+-------------+-----+---------+--------+\n",
            "|     217|       Gilroy|6:06a|    6:15a|       9|\n",
            "|     217|   San Martin|6:15a|    6:21a|       6|\n",
            "|     217|  Morgan Hill|6:21a|    6:36a|      15|\n",
            "|     217| Blossom Hill|6:36a|    6:42a|       6|\n",
            "|     217|      Capitol|6:42a|    6:50a|       8|\n",
            "|     217|       Tamien|6:50a|    6:59a|       9|\n",
            "|     217|     San Jose|6:59a|     null|    null|\n",
            "|     324|San Francisco|7:59a|    8:03a|       4|\n",
            "|     324|  22nd Street|8:03a|    8:16a|      13|\n",
            "|     324|     Millbrae|8:16a|    8:24a|       8|\n",
            "|     324|    Hillsdale|8:24a|    8:31a|       7|\n",
            "|     324| Redwood City|8:31a|    8:37a|       6|\n",
            "|     324|    Palo Alto|8:37a|    9:05a|      28|\n",
            "|     324|     San Jose|9:05a|     null|    null|\n",
            "+--------+-------------+-----+---------+--------+\n",
            "\n",
            "+--------+-------------+-----+--------+-------------+\n",
            "|train_id|      station| time|diff_min|running_total|\n",
            "+--------+-------------+-----+--------+-------------+\n",
            "|     217|       Gilroy|6:06a|       9|          9.0|\n",
            "|     217|   San Martin|6:15a|       6|         15.0|\n",
            "|     217|  Morgan Hill|6:21a|      15|         30.0|\n",
            "|     217| Blossom Hill|6:36a|       6|         36.0|\n",
            "|     217|      Capitol|6:42a|       8|         44.0|\n",
            "|     217|       Tamien|6:50a|       9|         53.0|\n",
            "|     217|     San Jose|6:59a|    null|         53.0|\n",
            "|     324|San Francisco|7:59a|       4|          4.0|\n",
            "|     324|  22nd Street|8:03a|      13|         17.0|\n",
            "|     324|     Millbrae|8:16a|       8|         25.0|\n",
            "|     324|    Hillsdale|8:24a|       7|         32.0|\n",
            "|     324| Redwood City|8:31a|       6|         38.0|\n",
            "|     324|    Palo Alto|8:37a|      28|         66.0|\n",
            "|     324|     San Jose|9:05a|    null|         66.0|\n",
            "+--------+-------------+-----+--------+-------------+\n",
            "\n",
            "+---+--------+-------------+-----+---------+\n",
            "|row|train_id|      station| time|time_next|\n",
            "+---+--------+-------------+-----+---------+\n",
            "|  1|     217|       Gilroy|6:06a|    6:15a|\n",
            "|  2|     217|   San Martin|6:15a|    6:21a|\n",
            "|  3|     217|  Morgan Hill|6:21a|    6:36a|\n",
            "|  4|     217| Blossom Hill|6:36a|    6:42a|\n",
            "|  5|     217|      Capitol|6:42a|    6:50a|\n",
            "|  6|     217|       Tamien|6:50a|    6:59a|\n",
            "|  7|     217|     San Jose|6:59a|     null|\n",
            "|  1|     324|San Francisco|7:59a|    8:03a|\n",
            "|  2|     324|  22nd Street|8:03a|    8:16a|\n",
            "|  3|     324|     Millbrae|8:16a|    8:24a|\n",
            "|  4|     324|    Hillsdale|8:24a|    8:31a|\n",
            "|  5|     324| Redwood City|8:31a|    8:37a|\n",
            "|  6|     324|    Palo Alto|8:37a|    9:05a|\n",
            "|  7|     324|     San Jose|9:05a|     null|\n",
            "+---+--------+-------------+-----+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V1zBfLSmdGv",
        "outputId": "33b80074-03b0-4663-c1e0-b60e2d2676b6"
      },
      "source": [
        "# Select column using col\r\n",
        "from pyspark.sql.functions import col\r\n",
        "df = spark.read.csv(\"trainsched.txt\", header=True, inferSchema=True, sep='\\t')\r\n",
        "df.createOrReplaceTempView(\"schedule\")\r\n",
        "df.select(col(\"train_id\"), col(\"station\")).show(3)\r\n",
        "df[[col(\"train_id\"), col(\"time\")]].show(3)\r\n",
        "\r\n",
        "# Rename column using col\r\n",
        "df[[col(\"train_id\").alias(\"train\"), \"station\"]].show(3)\r\n",
        "\r\n",
        "# Using SQL\r\n",
        "spark.sql(\"SELECT train_id AS train, station FROM schedule ORDER BY train_id LIMIT 3\").show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------+\n",
            "|train_id|      station|\n",
            "+--------+-------------+\n",
            "|     324|San Francisco|\n",
            "|     324|  22nd Street|\n",
            "|     324|     Millbrae|\n",
            "+--------+-------------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+--------+-----+\n",
            "|train_id| time|\n",
            "+--------+-----+\n",
            "|     324|7:59a|\n",
            "|     324|8:03a|\n",
            "|     324|8:16a|\n",
            "+--------+-----+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-----+-------------+\n",
            "|train|      station|\n",
            "+-----+-------------+\n",
            "|  324|San Francisco|\n",
            "|  324|  22nd Street|\n",
            "|  324|     Millbrae|\n",
            "+-----+-------------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-----+-----------+\n",
            "|train|    station|\n",
            "+-----+-----------+\n",
            "|  217|     Gilroy|\n",
            "|  217|Morgan Hill|\n",
            "|  217| San Martin|\n",
            "+-----+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mPuSA1gDF4b",
        "outputId": "cc929e0d-6817-4e29-8dee-56dcacb0acb3"
      },
      "source": [
        "# .agg in pyspark is only able to do a single aggregation on each column at a time.\r\n",
        "\r\n",
        "from pyspark.sql.functions import col\r\n",
        "df = spark.read.csv(\"trainsched.txt\", header=True, inferSchema=True, sep='\\t')\r\n",
        "df.createOrReplaceTempView(\"schedule\")\r\n",
        "spark.sql('SELECT train_id, MIN(time), MAX(time) FROM schedule GROUP BY train_id').show()\r\n",
        "df.groupBy('train_id').agg({'time':'min', 'time':'max'}).show()\r\n",
        "\r\n",
        "\r\n",
        "# Aggregating the same column twice\r\n",
        "from pyspark.sql.functions import min, max, col\r\n",
        "expr = [min(col(\"time\")).alias('start'), max(col(\"time\")).alias('end')]\r\n",
        "dot_df = df.groupBy(\"train_id\").agg(*expr)\r\n",
        "dot_df.show()\r\n",
        "\r\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---------+---------+\n",
            "|train_id|min(time)|max(time)|\n",
            "+--------+---------+---------+\n",
            "|     324|    7:59a|    9:05a|\n",
            "|     217|    6:06a|    6:59a|\n",
            "+--------+---------+---------+\n",
            "\n",
            "+--------+---------+\n",
            "|train_id|max(time)|\n",
            "+--------+---------+\n",
            "|     324|    9:05a|\n",
            "|     217|    6:59a|\n",
            "+--------+---------+\n",
            "\n",
            "+--------+-----+-----+\n",
            "|train_id|start|  end|\n",
            "+--------+-----+-----+\n",
            "|     324|7:59a|9:05a|\n",
            "|     217|6:06a|6:59a|\n",
            "+--------+-----+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttU4hkD9DnB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c52efaf-6768-465e-808b-4813b0211dd9"
      },
      "source": [
        "# Window functions using dot notation\r\n",
        "df = spark.read.csv(\"trainsched.txt\", header=True, inferSchema=True, sep='\\t')\r\n",
        "df.createOrReplaceTempView(\"schedule\")\r\n",
        "\r\n",
        "from pyspark.sql import Window\r\n",
        "from pyspark.sql.functions import row_number, lead\r\n",
        "df.withColumn(\"id\", row_number().over(Window.partitionBy(\"train_id\").orderBy(\"time\"))).show()\r\n",
        "df.withColumn(\"next\", lead(\"time\", 1).over(Window.partitionBy(\"train_id\").orderBy(\"time\"))).show()\r\n",
        "\r\n",
        "# Write a SQL query giving a result identical to dot_df\r\n",
        "query = \"SELECT train_id, MIN(time) AS start, MAX(time) AS end FROM schedule GROUP BY train_id\"\r\n",
        "sql_df = spark.sql(query)\r\n",
        "sql_df.show()\r\n",
        "\r\n",
        "# window function query uses dot notation\r\n",
        "from pyspark.sql.functions import unix_timestamp\r\n",
        "window = Window.partitionBy('train_id').orderBy('time')\r\n",
        "dot_df = df.withColumn('diff_min_cal', (unix_timestamp(lead('time', 1).over(window),'H:mm') - unix_timestamp('time', 'H:mm'))/60)\r\n",
        "dot_df.show()\r\n",
        "\r\n",
        "# Using SQL\r\n",
        "query = \"\"\"\r\n",
        "SELECT *, \r\n",
        "(UNIX_TIMESTAMP(LEAD(time, 1) OVER(PARTITION BY train_id ORDER BY time),'H:m')-UNIX_TIMESTAMP(time, 'H:m'))/60 AS diff_min_cal \r\n",
        "FROM schedule \r\n",
        "\"\"\"\r\n",
        "spark.sql(query).show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------+-----+--------+---+\n",
            "|train_id|      station| time|diff_min| id|\n",
            "+--------+-------------+-----+--------+---+\n",
            "|     324|San Francisco|7:59a|       4|  1|\n",
            "|     324|  22nd Street|8:03a|      13|  2|\n",
            "|     324|     Millbrae|8:16a|       8|  3|\n",
            "|     324|    Hillsdale|8:24a|       7|  4|\n",
            "|     324| Redwood City|8:31a|       6|  5|\n",
            "|     324|    Palo Alto|8:37a|      28|  6|\n",
            "|     324|     San Jose|9:05a|    null|  7|\n",
            "|     217|       Gilroy|6:06a|       9|  1|\n",
            "|     217|   San Martin|6:15a|       6|  2|\n",
            "|     217|  Morgan Hill|6:21a|      15|  3|\n",
            "|     217| Blossom Hill|6:36a|       6|  4|\n",
            "|     217|      Capitol|6:42a|       8|  5|\n",
            "|     217|       Tamien|6:50a|       9|  6|\n",
            "|     217|     San Jose|6:59a|    null|  7|\n",
            "+--------+-------------+-----+--------+---+\n",
            "\n",
            "+--------+-------------+-----+--------+-----+\n",
            "|train_id|      station| time|diff_min| next|\n",
            "+--------+-------------+-----+--------+-----+\n",
            "|     324|San Francisco|7:59a|       4|8:03a|\n",
            "|     324|  22nd Street|8:03a|      13|8:16a|\n",
            "|     324|     Millbrae|8:16a|       8|8:24a|\n",
            "|     324|    Hillsdale|8:24a|       7|8:31a|\n",
            "|     324| Redwood City|8:31a|       6|8:37a|\n",
            "|     324|    Palo Alto|8:37a|      28|9:05a|\n",
            "|     324|     San Jose|9:05a|    null| null|\n",
            "|     217|       Gilroy|6:06a|       9|6:15a|\n",
            "|     217|   San Martin|6:15a|       6|6:21a|\n",
            "|     217|  Morgan Hill|6:21a|      15|6:36a|\n",
            "|     217| Blossom Hill|6:36a|       6|6:42a|\n",
            "|     217|      Capitol|6:42a|       8|6:50a|\n",
            "|     217|       Tamien|6:50a|       9|6:59a|\n",
            "|     217|     San Jose|6:59a|    null| null|\n",
            "+--------+-------------+-----+--------+-----+\n",
            "\n",
            "+--------+-----+-----+\n",
            "|train_id|start|  end|\n",
            "+--------+-----+-----+\n",
            "|     324|7:59a|9:05a|\n",
            "|     217|6:06a|6:59a|\n",
            "+--------+-----+-----+\n",
            "\n",
            "+--------+-------------+-----+--------+------------+\n",
            "|train_id|      station| time|diff_min|diff_min_cal|\n",
            "+--------+-------------+-----+--------+------------+\n",
            "|     324|San Francisco|7:59a|       4|         4.0|\n",
            "|     324|  22nd Street|8:03a|      13|        13.0|\n",
            "|     324|     Millbrae|8:16a|       8|         8.0|\n",
            "|     324|    Hillsdale|8:24a|       7|         7.0|\n",
            "|     324| Redwood City|8:31a|       6|         6.0|\n",
            "|     324|    Palo Alto|8:37a|      28|        28.0|\n",
            "|     324|     San Jose|9:05a|    null|        null|\n",
            "|     217|       Gilroy|6:06a|       9|         9.0|\n",
            "|     217|   San Martin|6:15a|       6|         6.0|\n",
            "|     217|  Morgan Hill|6:21a|      15|        15.0|\n",
            "|     217| Blossom Hill|6:36a|       6|         6.0|\n",
            "|     217|      Capitol|6:42a|       8|         8.0|\n",
            "|     217|       Tamien|6:50a|       9|         9.0|\n",
            "|     217|     San Jose|6:59a|    null|        null|\n",
            "+--------+-------------+-----+--------+------------+\n",
            "\n",
            "+--------+-------------+-----+--------+------------+\n",
            "|train_id|      station| time|diff_min|diff_min_cal|\n",
            "+--------+-------------+-----+--------+------------+\n",
            "|     324|San Francisco|7:59a|       4|         4.0|\n",
            "|     324|  22nd Street|8:03a|      13|        13.0|\n",
            "|     324|     Millbrae|8:16a|       8|         8.0|\n",
            "|     324|    Hillsdale|8:24a|       7|         7.0|\n",
            "|     324| Redwood City|8:31a|       6|         6.0|\n",
            "|     324|    Palo Alto|8:37a|      28|        28.0|\n",
            "|     324|     San Jose|9:05a|    null|        null|\n",
            "|     217|       Gilroy|6:06a|       9|         9.0|\n",
            "|     217|   San Martin|6:15a|       6|         6.0|\n",
            "|     217|  Morgan Hill|6:21a|      15|        15.0|\n",
            "|     217| Blossom Hill|6:36a|       6|         6.0|\n",
            "|     217|      Capitol|6:42a|       8|         8.0|\n",
            "|     217|       Tamien|6:50a|       9|         9.0|\n",
            "|     217|     San Jose|6:59a|    null|        null|\n",
            "+--------+-------------+-----+--------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoJ8BTs5QTuK",
        "outputId": "303b3db6-88f4-42f2-c356-a8b0e62e0659"
      },
      "source": [
        "### The issue of time in this data set is the letter 'a'\r\n",
        "### Here is how to fix it without settng spark config.  \r\n",
        "### Note specifically the format string change of H:ma. The a represents either am or pm.\r\n",
        "### If you set config in Spark 3 then it doesn't matter whether it is H:ma or H:m.\r\n",
        "\r\n",
        "from pyspark.sql.functions import concat, lit\r\n",
        "df = spark.read.csv(\"trainsched.txt\", header=True, inferSchema=True, sep='\\t')\r\n",
        "df.withColumn('fixed_time', concat(df[\"time\"], lit('m'))).createOrReplaceTempView(\"schedule\")\r\n",
        "\r\n",
        "query = \"\"\"\r\n",
        "SELECT *, (UNIX_TIMESTAMP(LEAD(fixed_time, 1) OVER (PARTITION BY train_id ORDER BY fixed_time), 'H:ma') - UNIX_TIMESTAMP(fixed_time, 'H:ma'))/60 AS diff_min\r\n",
        "FROM schedule\r\n",
        "\"\"\"\r\n",
        "spark.sql(query).show()\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------------+-----+--------+----------+--------+\n",
            "|train_id|      station| time|diff_min|fixed_time|diff_min|\n",
            "+--------+-------------+-----+--------+----------+--------+\n",
            "|     324|San Francisco|7:59a|       4|    7:59am|     4.0|\n",
            "|     324|  22nd Street|8:03a|      13|    8:03am|    13.0|\n",
            "|     324|     Millbrae|8:16a|       8|    8:16am|     8.0|\n",
            "|     324|    Hillsdale|8:24a|       7|    8:24am|     7.0|\n",
            "|     324| Redwood City|8:31a|       6|    8:31am|     6.0|\n",
            "|     324|    Palo Alto|8:37a|      28|    8:37am|    28.0|\n",
            "|     324|     San Jose|9:05a|    null|    9:05am|    null|\n",
            "|     217|       Gilroy|6:06a|       9|    6:06am|     9.0|\n",
            "|     217|   San Martin|6:15a|       6|    6:15am|     6.0|\n",
            "|     217|  Morgan Hill|6:21a|      15|    6:21am|    15.0|\n",
            "|     217| Blossom Hill|6:36a|       6|    6:36am|     6.0|\n",
            "|     217|      Capitol|6:42a|       8|    6:42am|     8.0|\n",
            "|     217|       Tamien|6:50a|       9|    6:50am|     9.0|\n",
            "|     217|     San Jose|6:59a|    null|    6:59am|    null|\n",
            "+--------+-------------+-----+--------+----------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1axFDdllhi_q",
        "outputId": "4f5cae8b-8196-4fc4-8aca-42bb76bcd3e7"
      },
      "source": [
        "### Loading text data\r\n",
        "from pyspark.sql.functions import col, lower, regexp_replace, split, explode, length, monotonically_increasing_id, when\r\n",
        "df2 = spark.read.text(\"sherlock.txt\")\r\n",
        "df3 = spark.read.load(\"sherlock.parquet\")\r\n",
        "df2.createOrReplaceTempView(\"scherlock\")\r\n",
        "\r\n",
        "# Get the first row\r\n",
        "print(df2.first())\r\n",
        "\r\n",
        "# Count the number of rows\r\n",
        "print(df2.count())\r\n",
        "\r\n",
        "# Show the first 15 rows\r\n",
        "df2.show(15, truncate=False)\r\n",
        "\r\n",
        "# Lower case operation\r\n",
        "display(df2.select(lower(col(\"value\"))).first())\r\n",
        "\r\n",
        "print(\"\\nText file is stored as dataframe in one column of the following name:\")\r\n",
        "display(df2.columns)\r\n",
        "\r\n",
        "# Using regexp_replace: 1st arg is column name, 2nd arg is the pattern to be replaced, 3rd is what to replace.\r\n",
        "df2[[regexp_replace(\"value\", \"eBook\\.\", \"eBook\").alias(\"Renamed_column\")]].show(15, truncate=False)\r\n",
        "\r\n",
        "# Tokenizing text\r\n",
        "df2[[split(\"value\", '[ ]').alias('words')]].show(truncate=False)\r\n",
        "# df2[[split(\"value\", '').alias('words')]].show(truncate=False) ก็ได้\r\n",
        "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()#\"\r\n",
        "df3 = df2[[split(\"value\", '[ %s]'%punctuation).alias('words without punctuation')]]\r\n",
        "df3.show(truncate=False)\r\n",
        "\r\n",
        "# Explode \r\n",
        "df4 = df3[[explode(\"words without punctuation\").alias(\"word\")]]\r\n",
        "df4.show(truncate=False)\r\n",
        "\r\n",
        "# Removing empty rows\r\n",
        "nonblank_df = df4.where(length(\"word\")>0)\r\n",
        "nonblank_df.show()\r\n",
        "\r\n",
        "# Adding a row id column\r\n",
        "nonblank_df = nonblank_df[[\"word\", monotonically_increasing_id().alias(\"id\")]]\r\n",
        "nonblank_df.show()\r\n",
        "\r\n",
        "# Partitioning the data\r\n",
        "nonblank_df = nonblank_df.withColumn(\"title\", when(nonblank_df[\"id\"]<25000, \"Preface\").when(nonblank_df[\"id\"]<50000, \"Chapter 1\").when(nonblank_df[\"id\"]<75000, \"Chapter 2\").otherwise(\"Chapter 3\"))\r\n",
        "nonblank_df.show()\r\n",
        "\r\n",
        "# Repartion on a column\r\n",
        "print(\"\\n\", f\"Number of partitions before repartitioning = {nonblank_df.rdd.getNumPartitions()}.\")\r\n",
        "nonblank_df = nonblank_df.repartition(4, \"title\")\r\n",
        "print(\"\\n\", f\"Number of partitions after repartitioning = {nonblank_df.rdd.getNumPartitions()}.\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Row(value='The Project Gutenberg EBook of The Adventures of Sherlock Holmes')\n",
            "128457\n",
            "+--------------------------------------------------------------------+\n",
            "|value                                                               |\n",
            "+--------------------------------------------------------------------+\n",
            "|The Project Gutenberg EBook of The Adventures of Sherlock Holmes    |\n",
            "|by Sir Arthur Conan Doyle                                           |\n",
            "|(#15 in our series by Sir Arthur Conan Doyle)                       |\n",
            "|                                                                    |\n",
            "|Copyright laws are changing all over the world. Be sure to check the|\n",
            "|copyright laws for your country before downloading or redistributing|\n",
            "|this or any other Project Gutenberg eBook.                          |\n",
            "|                                                                    |\n",
            "|This header should be the first thing seen when viewing this Project|\n",
            "|Gutenberg file.  Please do not remove it.  Do not change or edit the|\n",
            "|header without written permission.                                  |\n",
            "|                                                                    |\n",
            "|Please read the \"legal small print,\" and other information about the|\n",
            "|eBook and Project Gutenberg at the bottom of this file.  Included is|\n",
            "|important information about your specific rights and restrictions in|\n",
            "+--------------------------------------------------------------------+\n",
            "only showing top 15 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Row(lower(value)='the project gutenberg ebook of the adventures of sherlock holmes')"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Text file is stored as dataframe in one column of the following name:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['value']"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------+\n",
            "|Renamed_column                                                      |\n",
            "+--------------------------------------------------------------------+\n",
            "|The Project Gutenberg EBook of The Adventures of Sherlock Holmes    |\n",
            "|by Sir Arthur Conan Doyle                                           |\n",
            "|(#15 in our series by Sir Arthur Conan Doyle)                       |\n",
            "|                                                                    |\n",
            "|Copyright laws are changing all over the world. Be sure to check the|\n",
            "|copyright laws for your country before downloading or redistributing|\n",
            "|this or any other Project Gutenberg eBook                           |\n",
            "|                                                                    |\n",
            "|This header should be the first thing seen when viewing this Project|\n",
            "|Gutenberg file.  Please do not remove it.  Do not change or edit the|\n",
            "|header without written permission.                                  |\n",
            "|                                                                    |\n",
            "|Please read the \"legal small print,\" and other information about the|\n",
            "|eBook and Project Gutenberg at the bottom of this file.  Included is|\n",
            "|important information about your specific rights and restrictions in|\n",
            "+--------------------------------------------------------------------+\n",
            "only showing top 15 rows\n",
            "\n",
            "+--------------------------------------------------------------------------------------+\n",
            "|words                                                                                 |\n",
            "+--------------------------------------------------------------------------------------+\n",
            "|[The, Project, Gutenberg, EBook, of, The, Adventures, of, Sherlock, Holmes]           |\n",
            "|[by, Sir, Arthur, Conan, Doyle]                                                       |\n",
            "|[(#15, in, our, series, by, Sir, Arthur, Conan, Doyle)]                               |\n",
            "|[]                                                                                    |\n",
            "|[Copyright, laws, are, changing, all, over, the, world., Be, sure, to, check, the]    |\n",
            "|[copyright, laws, for, your, country, before, downloading, or, redistributing]        |\n",
            "|[this, or, any, other, Project, Gutenberg, eBook.]                                    |\n",
            "|[]                                                                                    |\n",
            "|[This, header, should, be, the, first, thing, seen, when, viewing, this, Project]     |\n",
            "|[Gutenberg, file., , Please, do, not, remove, it., , Do, not, change, or, edit, the]  |\n",
            "|[header, without, written, permission.]                                               |\n",
            "|[]                                                                                    |\n",
            "|[Please, read, the, \"legal, small, print,\", and, other, information, about, the]      |\n",
            "|[eBook, and, Project, Gutenberg, at, the, bottom, of, this, file., , Included, is]    |\n",
            "|[important, information, about, your, specific, rights, and, restrictions, in]        |\n",
            "|[how, the, file, may, be, used., , You, can, also, find, out, about, how, to, make, a]|\n",
            "|[donation, to, Project, Gutenberg,, and, how, to, get, involved.]                     |\n",
            "|[]                                                                                    |\n",
            "|[]                                                                                    |\n",
            "|[**Welcome, To, The, World, of, Free, Plain, Vanilla, Electronic, Texts**]            |\n",
            "+--------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---------------------------------------------------------------------------------------+\n",
            "|words without punctuation                                                              |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "|[The, Project, Gutenberg, EBook, of, The, Adventures, of, Sherlock, Holmes]            |\n",
            "|[by, Sir, Arthur, Conan, Doyle]                                                        |\n",
            "|[, , 15, in, our, series, by, Sir, Arthur, Conan, Doyle, ]                             |\n",
            "|[]                                                                                     |\n",
            "|[Copyright, laws, are, changing, all, over, the, world, , Be, sure, to, check, the]    |\n",
            "|[copyright, laws, for, your, country, before, downloading, or, redistributing]         |\n",
            "|[this, or, any, other, Project, Gutenberg, eBook, ]                                    |\n",
            "|[]                                                                                     |\n",
            "|[This, header, should, be, the, first, thing, seen, when, viewing, this, Project]      |\n",
            "|[Gutenberg, file, , , Please, do, not, remove, it, , , Do, not, change, or, edit, the] |\n",
            "|[header, without, written, permission, ]                                               |\n",
            "|[]                                                                                     |\n",
            "|[Please, read, the, , legal, small, print, , , and, other, information, about, the]    |\n",
            "|[eBook, and, Project, Gutenberg, at, the, bottom, of, this, file, , , Included, is]    |\n",
            "|[important, information, about, your, specific, rights, and, restrictions, in]         |\n",
            "|[how, the, file, may, be, used, , , You, can, also, find, out, about, how, to, make, a]|\n",
            "|[donation, to, Project, Gutenberg, , and, how, to, get, involved, ]                    |\n",
            "|[]                                                                                     |\n",
            "|[]                                                                                     |\n",
            "|[, , Welcome, To, The, World, of, Free, Plain, Vanilla, Electronic, Texts, , ]         |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----------+\n",
            "|word      |\n",
            "+----------+\n",
            "|The       |\n",
            "|Project   |\n",
            "|Gutenberg |\n",
            "|EBook     |\n",
            "|of        |\n",
            "|The       |\n",
            "|Adventures|\n",
            "|of        |\n",
            "|Sherlock  |\n",
            "|Holmes    |\n",
            "|by        |\n",
            "|Sir       |\n",
            "|Arthur    |\n",
            "|Conan     |\n",
            "|Doyle     |\n",
            "|          |\n",
            "|          |\n",
            "|15        |\n",
            "|in        |\n",
            "|our       |\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----------+\n",
            "|      word|\n",
            "+----------+\n",
            "|       The|\n",
            "|   Project|\n",
            "| Gutenberg|\n",
            "|     EBook|\n",
            "|        of|\n",
            "|       The|\n",
            "|Adventures|\n",
            "|        of|\n",
            "|  Sherlock|\n",
            "|    Holmes|\n",
            "|        by|\n",
            "|       Sir|\n",
            "|    Arthur|\n",
            "|     Conan|\n",
            "|     Doyle|\n",
            "|        15|\n",
            "|        in|\n",
            "|       our|\n",
            "|    series|\n",
            "|        by|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----------+---+\n",
            "|      word| id|\n",
            "+----------+---+\n",
            "|       The|  0|\n",
            "|   Project|  1|\n",
            "| Gutenberg|  2|\n",
            "|     EBook|  3|\n",
            "|        of|  4|\n",
            "|       The|  5|\n",
            "|Adventures|  6|\n",
            "|        of|  7|\n",
            "|  Sherlock|  8|\n",
            "|    Holmes|  9|\n",
            "|        by| 10|\n",
            "|       Sir| 11|\n",
            "|    Arthur| 12|\n",
            "|     Conan| 13|\n",
            "|     Doyle| 14|\n",
            "|        15| 15|\n",
            "|        in| 16|\n",
            "|       our| 17|\n",
            "|    series| 18|\n",
            "|        by| 19|\n",
            "+----------+---+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----------+---+-------+\n",
            "|      word| id|  title|\n",
            "+----------+---+-------+\n",
            "|       The|  0|Preface|\n",
            "|   Project|  1|Preface|\n",
            "| Gutenberg|  2|Preface|\n",
            "|     EBook|  3|Preface|\n",
            "|        of|  4|Preface|\n",
            "|       The|  5|Preface|\n",
            "|Adventures|  6|Preface|\n",
            "|        of|  7|Preface|\n",
            "|  Sherlock|  8|Preface|\n",
            "|    Holmes|  9|Preface|\n",
            "|        by| 10|Preface|\n",
            "|       Sir| 11|Preface|\n",
            "|    Arthur| 12|Preface|\n",
            "|     Conan| 13|Preface|\n",
            "|     Doyle| 14|Preface|\n",
            "|        15| 15|Preface|\n",
            "|        in| 16|Preface|\n",
            "|       our| 17|Preface|\n",
            "|    series| 18|Preface|\n",
            "|        by| 19|Preface|\n",
            "+----------+---+-------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            " Number of partitions before repartitioning = 2.\n",
            "\n",
            " Number of partitions after repartitioning = 4.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPvy6zZ13_YP",
        "outputId": "dc591b0f-c31e-4c7d-97a0-3b21b63c790e"
      },
      "source": [
        "# A moving window query\r\n",
        "### Loading text data\r\n",
        "from pyspark.sql.functions import col, lower, regexp_replace, split, explode, length, monotonically_increasing_id, when\r\n",
        "df2 = spark.read.text(\"sherlock.txt\")\r\n",
        "df2.createOrReplaceTempView(\"scherlock\")\r\n",
        "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()#\"\r\n",
        "df3 = df2[[split(\"value\", '[ %s]'%punctuation).alias('words')]]\r\n",
        "df4 = df3[[explode(\"words\").alias(\"word\")]]\r\n",
        "df = df4.where(length(\"word\")>0)\r\n",
        "df = df[[\"word\", monotonically_increasing_id().alias(\"id\")]]\r\n",
        "df = df.withColumn(\"part\", when(df[\"id\"]<92210, 1).when(df[\"id\"]<2*92210, 2).when(df[\"id\"]<3*92210, 3).when(df[\"id\"]<4*92210, 4).when(df[\"id\"]<5*92210, 5).when(df[\"id\"]<6*92210, 6).when(df[\"id\"]<7*92210, 7).when(df[\"id\"]<8*92210, 8).when(df[\"id\"]<9*92210, 9).when(df[\"id\"]<10*92210, 10).when(df[\"id\"]<11*92210, 11).otherwise(12))\r\n",
        "print(\"\\n\", f\"Number of partitions before repartitioning = {df.rdd.getNumPartitions()}.\")\r\n",
        "df = df.repartition(12, \"part\")\r\n",
        "print(\"\\n\", f\"Number of partitions after repartitioning = {df.rdd.getNumPartitions()}.\")\r\n",
        "df.show(3)\r\n",
        "df.createOrReplaceTempView(\"df\")\r\n",
        "query = \"\"\"SELECT id, \r\n",
        "                  word AS w1,\r\n",
        "                  LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\r\n",
        "                  LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w3\r\n",
        "           FROM df\"\"\"\r\n",
        "spark.sql(query).sort(\"id\").show(3)\r\n",
        "\r\n",
        "print(\"Using lag query\")\r\n",
        "lag_query = \"\"\"SELECT id, \r\n",
        "                      LAG(word, 2) OVER(PARTITION BY part ORDER BY id) AS w1,\r\n",
        "                      LAG(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\r\n",
        "                      word AS w3\r\n",
        "                      FROM df\r\n",
        "                      ORDER BY id\"\"\"\r\n",
        "spark.sql(lag_query).show(3)\r\n",
        "\r\n",
        "# Suppose we want to look at part 2\r\n",
        "lag_query = \"\"\"SELECT id, \r\n",
        "                      LAG(word, 2) OVER(PARTITION BY part ORDER BY id) AS w1,\r\n",
        "                      LAG(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\r\n",
        "                      word AS w3\r\n",
        "                      FROM df\r\n",
        "                      WHERE part = 2\r\n",
        "                      ORDER BY id\"\"\"\r\n",
        "spark.sql(lag_query).show(3)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Number of partitions before repartitioning = 2.\n",
            "\n",
            " Number of partitions after repartitioning = 12.\n",
            "+---------+------+----+\n",
            "|     word|    id|part|\n",
            "+---------+------+----+\n",
            "|flattened|461050|   6|\n",
            "|   nodule|461051|   6|\n",
            "|       in|461052|   6|\n",
            "+---------+------+----+\n",
            "only showing top 3 rows\n",
            "\n",
            "+---+---------+---------+---------+\n",
            "| id|       w1|       w2|       w3|\n",
            "+---+---------+---------+---------+\n",
            "|  0|      The|  Project|Gutenberg|\n",
            "|  1|  Project|Gutenberg|    EBook|\n",
            "|  2|Gutenberg|    EBook|       of|\n",
            "+---+---------+---------+---------+\n",
            "only showing top 3 rows\n",
            "\n",
            "Using lag query\n",
            "+---+----+-------+---------+\n",
            "| id|  w1|     w2|       w3|\n",
            "+---+----+-------+---------+\n",
            "|  0|null|   null|      The|\n",
            "|  1|null|    The|  Project|\n",
            "|  2| The|Project|Gutenberg|\n",
            "+---+----+-------+---------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-----+------+-------+-------+\n",
            "|   id|    w1|     w2|     w3|\n",
            "+-----+------+-------+-------+\n",
            "|92210|  null|   null| helped|\n",
            "|92211|  null| helped|himself|\n",
            "|92212|helped|himself|     to|\n",
            "+-----+------+-------+-------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOwCGOn25Cn2",
        "outputId": "932d7c31-3ae2-49b2-c3c3-62dcb8382083"
      },
      "source": [
        "from pyspark.sql.functions import col, lower, regexp_replace, split, explode, length, monotonically_increasing_id, when\r\n",
        "df2 = spark.read.text(\"sherlock.txt\")\r\n",
        "df2.createOrReplaceTempView(\"scherlock\")\r\n",
        "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()#\"\r\n",
        "df3 = df2[[split(\"value\", '[ %s]'%punctuation).alias('words')]]\r\n",
        "df4 = df3[[explode(\"words\").alias(\"word\")]]\r\n",
        "df = df4.where(length(\"word\")>0)\r\n",
        "df = df[[\"word\", monotonically_increasing_id().alias(\"id\")]]\r\n",
        "df = df.withColumn(\"part\", when(df[\"id\"]<92210, 1).when(df[\"id\"]<2*92210, 2).when(df[\"id\"]<3*92210, 3).when(df[\"id\"]<4*92210, 4).when(df[\"id\"]<5*92210, 5).when(df[\"id\"]<6*92210, 6).when(df[\"id\"]<7*92210, 7).when(df[\"id\"]<8*92210, 8).when(df[\"id\"]<9*92210, 9).when(df[\"id\"]<10*92210, 10).when(df[\"id\"]<11*92210, 11).otherwise(12))\r\n",
        "print(\"\\n\", f\"Number of partitions before repartitioning = {df.rdd.getNumPartitions()}.\")\r\n",
        "df = df.repartition(12, \"part\")\r\n",
        "print(\"\\n\", f\"Number of partitions after repartitioning = {df.rdd.getNumPartitions()}.\")\r\n",
        "df.show(3)\r\n",
        "df.createOrReplaceTempView(\"df\")\r\n",
        "\r\n",
        "query = \"\"\"SELECT w1, w2, w3, COUNT(*) as count\r\n",
        "           FROM (SELECT word AS w1,\r\n",
        "                        LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\r\n",
        "                        LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w3\r\n",
        "                 FROM df)\r\n",
        "           GROUP BY w1, w2, w3\r\n",
        "           ORDER BY count DESC\"\"\"\r\n",
        "\r\n",
        "spark.sql(query).show()\r\n",
        "\r\n",
        "query = \"\"\"SELECT w1, w2, w3, length(w1)+length(w2)+length(w3) as length\r\n",
        "           FROM (SELECT word AS w1,\r\n",
        "                        LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\r\n",
        "                        LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w3\r\n",
        "                 FROM df\r\n",
        "                 WHERE part <> 1 AND part <> 2 AND part <> 12) \r\n",
        "           GROUP BY w1, w2, w3\r\n",
        "           ORDER BY length DESC\"\"\"\r\n",
        "\r\n",
        "spark.sql(query).show()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Number of partitions before repartitioning = 2.\n",
            "\n",
            " Number of partitions after repartitioning = 12.\n",
            "+---------+------+----+\n",
            "|     word|    id|part|\n",
            "+---------+------+----+\n",
            "|flattened|461050|   6|\n",
            "|   nodule|461051|   6|\n",
            "|       in|461052|   6|\n",
            "+---------+------+----+\n",
            "only showing top 3 rows\n",
            "\n",
            "+---------+---------+------+-----+\n",
            "|       w1|       w2|    w3|count|\n",
            "+---------+---------+------+-----+\n",
            "|      the|   United|States|  397|\n",
            "|      one|       of|   the|  335|\n",
            "|      out|       of|   the|  245|\n",
            "|       of|      the|United|  235|\n",
            "|        I|      don|     t|  217|\n",
            "|     that|       he|   was|  194|\n",
            "|     that|       it|   was|  181|\n",
            "|      and|       in|   the|  180|\n",
            "|      met|     with|    in|  174|\n",
            "|     part|       of|   the|  160|\n",
            "|       he|      did|   not|  159|\n",
            "|       up|       to|   the|  159|\n",
            "|      the|formation|    of|  152|\n",
            "|       in|    front|    of|  145|\n",
            "|commander|       in| chief|  144|\n",
            "|       as|     well|    as|  144|\n",
            "|      the|      end|    of|  144|\n",
            "|      the|     fact|  that|  142|\n",
            "|      the|     same|  time|  140|\n",
            "|       it|      may|    be|  138|\n",
            "+---------+---------+------+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------------+--------------------+--------------------+------+\n",
            "|                  w1|                  w2|                  w3|length|\n",
            "+--------------------+--------------------+--------------------+------+\n",
            "|             Husband|+-----------+----...|+-------+--------...|   107|\n",
            "|+-----------+----...|+-------+--------...|                   M|   101|\n",
            "|             Husband|             Husband|+-----------+----...|    86|\n",
            "|Arterio-sclerosis...|           Varieties|--VARIX--ANGIOMAT...|    84|\n",
            "|                 net|+----------------...|         Transcriber|    84|\n",
            "|+----------------...|              STATES|          POPULATION|    84|\n",
            "|-----------------...|          FOOTNOTES:|                  21|    83|\n",
            "|-----------------...|              United|              States|    83|\n",
            "|+----------------...|              OXFORD|             MEDICAL|    83|\n",
            "|                  25|-----------------...|          FOOTNOTES:|    83|\n",
            "|+----------------...|         Transcriber|                   s|    82|\n",
            "|                 531|+----------------...|          FOOTNOTES:|    81|\n",
            "|             version|                only|+----------------...|    81|\n",
            "|                1910|-----------------...|              United|    81|\n",
            "|                only|+----------------...|              OXFORD|    80|\n",
            "|+----------------...|              United|              States|    80|\n",
            "|                1910|-----------------...|                AREA|    79|\n",
            "|+----------------...|          FOOTNOTES:|                   3|    79|\n",
            "|                1920|                1910|-----------------...|    79|\n",
            "|-----------------...|                AREA|                1920|    79|\n",
            "+--------------------+--------------------+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgg87pWEWmeH",
        "outputId": "8f8b1bdb-e66b-4558-f90c-4d8eea6dd81a"
      },
      "source": [
        "query = \"\"\"SELECT w1, w2, w3, w4, w5, COUNT(*) AS count \r\n",
        "           FROM (SELECT word AS w1,\r\n",
        "                        LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\r\n",
        "                        LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w3,\r\n",
        "                        LEAD(word, 3) OVER(PARTITION BY part ORDER BY id) AS w4,\r\n",
        "                        LEAD(word, 4) OVER(PARTITION BY part ORDER BY id) AS w5\r\n",
        "                 FROM df)\r\n",
        "GROUP BY w1, w2, w3, w4, w5\r\n",
        "ORDER BY count DESC\r\n",
        "LIMIT 10\r\n",
        "\"\"\"\r\n",
        "spark.sql(query).show()\r\n",
        "\r\n",
        "# Unique 5-tuples sorted in descending order\r\n",
        "query = \"\"\"\r\n",
        "SELECT DISTINCT w1, w2, w3, w4, w5 FROM (\r\n",
        "   SELECT word AS w1,\r\n",
        "   LEAD(word, 1) OVER(PARTITION BY part ORDER BY id ) AS w2,\r\n",
        "   LEAD(word, 2) OVER(PARTITION BY part ORDER BY id ) AS w3,\r\n",
        "   LEAD(word, 3) OVER(PARTITION BY part ORDER BY id ) AS w4,\r\n",
        "   LEAD(word, 4) OVER(PARTITION BY part ORDER BY id ) AS w5\r\n",
        "   FROM df\r\n",
        ")\r\n",
        "ORDER BY w1 DESC, w2 DESC, w3 DESC, w4 DESC, w5 DESC \r\n",
        "LIMIT 10\r\n",
        "\"\"\"\r\n",
        "spark.sql(query).show()\r\n",
        "\r\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+---------+--------+-------+----------+-----+\n",
            "|     w1|       w2|      w3|     w4|        w5|count|\n",
            "+-------+---------+--------+-------+----------+-----+\n",
            "|History|       of|     the| United|    States|   57|\n",
            "|     in|      the|  region|     of|       the|   36|\n",
            "|Project|Gutenberg|Literary|Archive|Foundation|   35|\n",
            "|     of|      the|  United| States|        pp|   31|\n",
            "|     in|      the|  middle|     of|       the|   27|\n",
            "|    the|    other|    side|     of|       the|   25|\n",
            "|     on|      the|    same|  lines|        as|   25|\n",
            "|     in|      the|    case|     of|       the|   25|\n",
            "|     up|      and|    down|    the|      room|   23|\n",
            "|    and|       at|     the|   same|      time|   23|\n",
            "+-------+---------+--------+-------+----------+-----+\n",
            "\n",
            "+---------+---------+-------+----------+------------+\n",
            "|       w1|       w2|     w3|        w4|          w5|\n",
            "+---------+---------+-------+----------+------------+\n",
            "|        ~| asterisk|    and| underline|  characters|\n",
            "|zygomatic|      and|frontal|     bones|         the|\n",
            "|   zygoma|       in|  front|        of|         the|\n",
            "|       zu|     sein|   Vera|        at|         the|\n",
            "|       zu|schwachen|     so|      kann|         man|\n",
            "|  zoology|      was|    not|    merely|acknowledged|\n",
            "|  zoology|      for|     in|         a|        frog|\n",
            "|  zoology|      and|     so|        on|        just|\n",
            "|zone--not|      the|    red|margin--an|  artificial|\n",
            "|     zone|    which|   lies|     about|        half|\n",
            "+---------+---------+-------+----------+------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SIwFdPf5Pyq",
        "outputId": "c417371c-f8fa-4e39-ed86-0dd34142cab7"
      },
      "source": [
        "#   Most frequent 3-tuple per chapter\r\n",
        "subquery = \"\"\"SELECT part, w1, w2, w3, COUNT(*) as count \r\n",
        "              FROM(SELECT part, \r\n",
        "                          word AS w1,\r\n",
        "                          LEAD(word, 1) OVER(PARTITION BY part ORDER BY id ) AS w2,\r\n",
        "                          LEAD(word, 2) OVER(PARTITION BY part ORDER BY id ) AS w3\r\n",
        "                          FROM df)\r\n",
        "              GROUP BY part, w1, w2, w3\r\n",
        "              ORDER BY part, count DESC\"\"\"\r\n",
        "spark.sql(subquery).show()\r\n",
        "\r\n",
        "query = \"\"\"SELECT part, w1, w2, w3, count \r\n",
        "           FROM (SELECT part,\r\n",
        "                        ROW_NUMBER() OVER (PARTITION BY part ORDER BY count DESC) AS row,\r\n",
        "                        w1, w2, w3, count\r\n",
        "                 FROM ( %s ))\r\n",
        "           WHERE row = 1\r\n",
        "           ORDER BY part ASC\"\"\" %subquery\r\n",
        "spark.sql(query).show()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+-----+-----+-----+\n",
            "|part|   w1|   w2|   w3|count|\n",
            "+----+-----+-----+-----+-----+\n",
            "|   1|    I|think| that|   42|\n",
            "|   1|  one|   of|  the|   38|\n",
            "|   1|  out|   of|  the|   33|\n",
            "|   1| that|   it|  was|   31|\n",
            "|   1| that|    I| have|   29|\n",
            "|   1| Lord|   St|Simon|   28|\n",
            "|   1|   It|  was|    a|   28|\n",
            "|   1| that|   it|   is|   27|\n",
            "|   1|    I|   do|  not|   27|\n",
            "|   1| that|   he|  was|   26|\n",
            "|   1|   It|   is|    a|   24|\n",
            "|   1|think| that|    I|   23|\n",
            "|   1| that|   he|  had|   23|\n",
            "|   1|    I| have| been|   21|\n",
            "|   1| that|    I|   am|   21|\n",
            "|   1|    I| have|   no|   21|\n",
            "|   1| that|    I|  had|   21|\n",
            "|   1|    I|could|  not|   21|\n",
            "|   1|    a|  man|  who|   20|\n",
            "|   1| that|    I|  was|   20|\n",
            "+----+-----+-----+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----+---------+------+------+-----+\n",
            "|part|       w1|    w2|    w3|count|\n",
            "+----+---------+------+------+-----+\n",
            "|   1|        I| think|  that|   42|\n",
            "|   2|      the|United|States|   88|\n",
            "|   3|      the|United|States|  169|\n",
            "|   4|      the|United|States|  129|\n",
            "|   5|      met|  with|    in|   91|\n",
            "|   6|       of|   the|  bone|   73|\n",
            "|   7|commander|    in| chief|   48|\n",
            "|   8|      one|    of|   the|   36|\n",
            "|   9|        I|   don|     t|   47|\n",
            "|  10|commander|    in| chief|   46|\n",
            "|  12|       of|   the|French|   61|\n",
            "+----+---------+------+------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "XtOfXtuR9x39",
        "outputId": "a7669a18-5b1b-42f4-c7d9-af95cb97c145"
      },
      "source": [
        "### Caching\r\n",
        "df = spark.read.csv(\"trainsched.txt\", header=True, inferSchema=True, sep='\\t')\r\n",
        "# from pyspark.sql.functions import col, lower, regexp_replace, split, explode, length, monotonically_increasing_id, when\r\n",
        "# df2 = spark.read.text(\"sherlock.txt\")\r\n",
        "# df2.createOrReplaceTempView(\"scherlock\")\r\n",
        "# punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()#\"\r\n",
        "# df3 = df2[[split(\"value\", '[ %s]'%punctuation).alias('words')]]\r\n",
        "# df4 = df3[[explode(\"words\").alias(\"word\")]]\r\n",
        "# df = df4.where(length(\"word\")>0)\r\n",
        "# df = df[[\"word\", monotonically_increasing_id().alias(\"id\")]]\r\n",
        "# df = df.withColumn(\"part\", when(df[\"id\"]<92210, 1).when(df[\"id\"]<2*92210, 2).when(df[\"id\"]<3*92210, 3).when(df[\"id\"]<4*92210, 4).when(df[\"id\"]<5*92210, 5).when(df[\"id\"]<6*92210, 6).when(df[\"id\"]<7*92210, 7).when(df[\"id\"]<8*92210, 8).when(df[\"id\"]<9*92210, 9).when(df[\"id\"]<10*92210, 10).when(df[\"id\"]<11*92210, 11).otherwise(12))\r\n",
        "# print(f\"\\nNumber of partitions before repartitioning = {df.rdd.getNumPartitions()}.\")\r\n",
        "# df = df.repartition(12, \"part\")\r\n",
        "# print(f\"\\nNumber of partitions after repartitioning = {df.rdd.getNumPartitions()}.\")\r\n",
        "print(f\"Is df cached? {df.is_cached}.\")\r\n",
        "display(df.storageLevel)\r\n",
        "df.cache()\r\n",
        "print(f\"Is df cached? {df.is_cached}.\")\r\n",
        "display(df.storageLevel)\r\n",
        "df.unpersist()\r\n",
        "print(f\"Is df cached? {df.is_cached}.\")\r\n",
        "display(df.storageLevel)\r\n",
        "\r\n",
        "# When memory is scarce, use this\r\n",
        "import pyspark\r\n",
        "print(\"\\nWhen memory is scarce, use this\")\r\n",
        "df.persist(storageLevel=pyspark.StorageLevel.MEMORY_AND_DISK)\r\n",
        "display(df.storageLevel)\r\n",
        "print(f\"Is df cached? {df.is_cached}.\")\r\n",
        "\r\n",
        "# Caching table in spark session\r\n",
        "df.createOrReplaceTempView(\"df\")\r\n",
        "display(spark.catalog.isCached(tableName='df'))\r\n",
        "spark.catalog.cacheTable('df')\r\n",
        "display(spark.catalog.isCached(tableName='df'))\r\n",
        "spark.catalog.uncacheTable('df')\r\n",
        "display(spark.catalog.isCached(tableName='df'))\r\n",
        "\r\n",
        "# Remove all cached tables\r\n",
        "spark.catalog.clearCache()\r\n",
        "print(\"Tables:\\n\", spark.catalog.listTables())"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is df cached? False.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "StorageLevel(False, False, False, False, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Is df cached? True.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "StorageLevel(True, True, False, True, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Is df cached? False.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "StorageLevel(False, False, False, False, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "When memory is scarce, use this\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "StorageLevel(True, True, False, False, 1)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Is df cached? True.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tables:\n",
            " [Table(name='df', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='schedule', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='scherlock', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "sfQSY1H7LNyW",
        "outputId": "cc02b949-03d9-440a-fef8-31001af2fec3"
      },
      "source": [
        "df = spark.read.csv(\"trainsched.txt\", header=True, inferSchema=True, sep='\\t')\r\n",
        "df.createOrReplaceTempView(\"schedule\")\r\n",
        "display(spark.catalog.listTables())\r\n",
        "spark.catalog.dropTempView(\"schedule\")\r\n",
        "display(spark.catalog.listTables())\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[Table(name='schedule', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glgDm28mOSxl",
        "outputId": "08f0153e-313c-46c6-ebd1-a51a9ff761e2"
      },
      "source": [
        "import logging\r\n",
        "import sys\r\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\r\n",
        "logging.info(\"Hello %s\", \"world\")\r\n",
        "logging.debug(\"Hello, take %d\", 2)\r\n",
        "\r\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-31 14:14:39,891 - INFO - Hello world\n",
            "2021-01-31 14:14:39,894 - INFO - Hello world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EEdZtQhyru9",
        "outputId": "afeb1d30-0a32-4834-a859-94460149d4d6"
      },
      "source": [
        "# ทำไม บรรทัดที่สองไม่ออก ?\r\n",
        "\r\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\r\n",
        "logging.info(\"Hello %s\", \"world\")\r\n",
        "logging.debug(\"Hello, take %d\", 2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-31 14:15:57,768 - INFO - Hello world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "9EQluGqBy9nj",
        "outputId": "931d163d-29ff-4056-ea25-583375a38107"
      },
      "source": [
        "df = spark.read.csv(\"trainsched.txt\", header=True, inferSchema=True, sep='\\t')\r\n",
        "df.createOrReplaceTempView(\"schedule\")\r\n",
        "display(spark.sql(\"EXPLAIN SELECT * FROM schedule\").first())\r\n",
        "df.explain()\r\n",
        "\r\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Row(plan='== Physical Plan ==\\nFileScan csv [train_id#664,station#665,time#666,diff_min#667] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/content/trainsched.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<train_id:int,station:string,time:string,diff_min:string>\\n\\n')"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "FileScan csv [train_id#664,station#665,time#666,diff_min#667] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/content/trainsched.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<train_id:int,station:string,time:string,diff_min:string>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGSOQ5Fx0dk4",
        "outputId": "146a54ea-c3e1-4db5-b723-e961bccf3e66"
      },
      "source": [
        "from pyspark.sql.functions import udf\r\n",
        "from pyspark.sql.types import BooleanType, StringType, IntegerType, FloatType, ArrayType\r\n",
        "from pyspark.sql.functions import col, lower, regexp_replace, split, explode, length, monotonically_increasing_id, when, size\r\n",
        "df = spark.read.text(\"sherlock.txt\")\r\n",
        "df.createOrReplaceTempView(\"scherlock\")\r\n",
        "short_udf = udf(lambda x: True if not x or len(x)<10 else False, BooleanType())\r\n",
        "df[[short_udf(\"value\").alias(\"short_udf\")]].show(5)\r\n",
        "df.show(5, truncate=False)\r\n",
        "\r\n",
        "# Try to replicate the data as close as that in the exercise.  Not 100% match.\r\n",
        "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()#\"\r\n",
        "df = df[[split(\"value\", '[ %s]'%punctuation).alias('words')]]\r\n",
        "df.show(5, truncate=False)\r\n",
        "\r\n",
        "# Creating an array udf\r\n",
        "def remove(x):\r\n",
        "    for i in x:\r\n",
        "        if i == \"\":\r\n",
        "            x.remove(i)\r\n",
        "    return x\r\n",
        "remove_udf = udf(remove, ArrayType(StringType()))\r\n",
        "# in_udf = udf(lambda x: x[0:len(x)-1] if x and len(x)>1 and x[-1] != \"\" else [], ArrayType(StringType()))\r\n",
        "# df = df.withColumn(\"words\", in_udf(\"words\"))\r\n",
        "df = df.withColumn(\"words\", remove_udf(\"words\"))\r\n",
        "df = df.where(size(df[\"words\"]) > 0)\r\n",
        "df[[\"words\", size(df[\"words\"])]].show(truncate=False)\r\n",
        "\r\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|short_udf|\n",
            "+---------+\n",
            "|    false|\n",
            "|    false|\n",
            "|    false|\n",
            "|     true|\n",
            "|    false|\n",
            "+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------------------------------------------------------+\n",
            "|value                                                               |\n",
            "+--------------------------------------------------------------------+\n",
            "|The Project Gutenberg EBook of The Adventures of Sherlock Holmes    |\n",
            "|by Sir Arthur Conan Doyle                                           |\n",
            "|(#15 in our series by Sir Arthur Conan Doyle)                       |\n",
            "|                                                                    |\n",
            "|Copyright laws are changing all over the world. Be sure to check the|\n",
            "+--------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------------------------------------------------------------------------+\n",
            "|words                                                                              |\n",
            "+-----------------------------------------------------------------------------------+\n",
            "|[The, Project, Gutenberg, EBook, of, The, Adventures, of, Sherlock, Holmes]        |\n",
            "|[by, Sir, Arthur, Conan, Doyle]                                                    |\n",
            "|[, , 15, in, our, series, by, Sir, Arthur, Conan, Doyle, ]                         |\n",
            "|[]                                                                                 |\n",
            "|[Copyright, laws, are, changing, all, over, the, world, , Be, sure, to, check, the]|\n",
            "+-----------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------------------------------------------------------------------------+-----------+\n",
            "|words                                                                              |size(words)|\n",
            "+-----------------------------------------------------------------------------------+-----------+\n",
            "|[The, Project, Gutenberg, EBook, of, The, Adventures, of, Sherlock, Holmes]        |10         |\n",
            "|[by, Sir, Arthur, Conan, Doyle]                                                    |5          |\n",
            "|[15, in, our, series, by, Sir, Arthur, Conan, Doyle]                               |9          |\n",
            "|[Copyright, laws, are, changing, all, over, the, world, Be, sure, to, check, the]  |13         |\n",
            "|[copyright, laws, for, your, country, before, downloading, or, redistributing]     |9          |\n",
            "|[this, or, any, other, Project, Gutenberg, eBook]                                  |7          |\n",
            "|[This, header, should, be, the, first, thing, seen, when, viewing, this, Project]  |12         |\n",
            "|[Gutenberg, file, Please, do, not, remove, it, , Do, not, change, or, edit, the]   |14         |\n",
            "|[header, without, written, permission]                                             |4          |\n",
            "|[Please, read, the, legal, small, print, and, other, information, about, the]      |11         |\n",
            "|[eBook, and, Project, Gutenberg, at, the, bottom, of, this, file, Included, is]    |12         |\n",
            "|[important, information, about, your, specific, rights, and, restrictions, in]     |9          |\n",
            "|[how, the, file, may, be, used, You, can, also, find, out, about, how, to, make, a]|16         |\n",
            "|[donation, to, Project, Gutenberg, and, how, to, get, involved]                    |9          |\n",
            "|[Welcome, To, The, World, of, Free, Plain, Vanilla, Electronic, Texts, ]           |11         |\n",
            "|[eBooks, Readable, By, Both, Humans, and, By, Computers, Since, 1971, ]            |11         |\n",
            "|[These, eBooks, Were, Prepared, By, Thousands, of, Volunteers, , ]                 |10         |\n",
            "|[Title:, The, Adventures, of, Sherlock, Holmes]                                    |6          |\n",
            "|[Author:, Sir, Arthur, Conan, Doyle]                                               |5          |\n",
            "|[Release, Date:, March, 1999, EBook, 1661]                                         |6          |\n",
            "+-----------------------------------------------------------------------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJYCsV5U4Cor",
        "outputId": "fb59020d-682b-41bb-d6fc-08efe9f9a708"
      },
      "source": [
        "list(set(1, 2, 3, 3, 4, 5, 3, 1, 2) - )\r\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function length in module pyspark.sql.functions:\n",
            "\n",
            "length(col)\n",
            "    Computes the character length of string data or number of bytes of binary data.\n",
            "    The length of character data includes the trailing spaces. The length of binary data\n",
            "    includes binary zeros.\n",
            "    \n",
            "    >>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n",
            "    [Row(length=4)]\n",
            "    \n",
            "    .. versionadded:: 1.5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-p0jcmY5uPs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}